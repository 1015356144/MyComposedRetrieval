"""
Iterative Trainer for Composed Image Retrieval
Implements iterative hard negative mining and knowledge distillation from foundation models
"""

import os
import json
import time
import torch
import torch.distributed as dist
import logging
from typing import Dict, List, Optional, Any
from torch.utils.data import DataLoader
from transformers import TrainingArguments

from .trainer import MMEBTrainer
from .model.model import MMEBModel
from .data.dataset.composed_retrieval_dataset import IterativeCIRRDataset, IterativeFashionIQDataset
from .utils import print_rank, print_master

logger = logging.getLogger(__name__)


class IterativeRetrievalTrainer(MMEBTrainer):
    """
    Trainer for iterative composed image retrieval with hard negative mining
    """
    
    def __init__(self, 
                 foundation_model=None, # 忽略传进来的实例
                 foundation_model_name: str = None,     # 只保存名字
                 max_iterations: int = 3,
                 hard_neg_collection_freq: int = 1,
                 caption_generation_batch_size: int = 8,
                 model_args=None,
                 data_args=None,
                 max_length=None,
                 # Fast mode and production mode parameters
                 fast_mode: bool = False,
                 fast_mode_max_samples: int = 100,
                 fast_mode_retrieval_db_size: int = 50,
                 fast_mode_max_steps: int = 5,
                 steps_per_iteration: int = 1000,  # New parameter: steps per iteration
                 production_save_steps: int = 100,
                 # Backward compatibility
                 production_max_steps: Optional[int] = None,  # Old parameter name for compatibility
                 **kwargs):
        # Store model_args, data_args and max_length before calling super().__init__()
        self.model_args = model_args
        self.data_args = data_args
        self.max_length = max_length
        
        # Handle parameter compatibility: steps_per_iteration vs production_max_steps
        if production_max_steps is not None:
            # Backward compatibility: use old parameter name and issue warning
            print_master("⚠️  WARNING: 'production_max_steps' is deprecated, use 'steps_per_iteration' instead")
            self.production_max_steps = production_max_steps
        else:
            # Use new parameter name
            self.production_max_steps = steps_per_iteration
        
        # Store fast mode and production mode settings
        self.fast_mode = fast_mode
        self.fast_mode_max_samples = fast_mode_max_samples
        self.fast_mode_retrieval_db_size = fast_mode_retrieval_db_size
        self.fast_mode_max_steps = fast_mode_max_steps
        # self.production_max_steps already set above
        self.production_save_steps = production_save_steps
        
        # Calculate iterative training step planning
        if self.fast_mode:
            self.steps_per_iteration = self.fast_mode_max_steps
        else:
            self.steps_per_iteration = self.production_max_steps
        
        # New strategy: independent training per iteration, no global training plan
        # Removed total_planned_steps concept, each iteration resets optimizer and scheduler
        print_master(f"📋 Training plan: {max_iterations} iterations × {self.steps_per_iteration} steps per iteration")
        print_master("🔄 Strategy: Each iteration will reset optimizer and LR scheduler for independent training")
        
        # Remove parameters that parent Trainer doesn't accept
        kwargs.pop('model_args', None)
        kwargs.pop('data_args', None)
        kwargs.pop('max_length', None)
        kwargs.pop('fast_mode', None)
        kwargs.pop('fast_mode_max_samples', None)
        kwargs.pop('fast_mode_retrieval_db_size', None)
        kwargs.pop('fast_mode_max_steps', None)
        kwargs.pop('steps_per_iteration', None)  # New parameter name
        kwargs.pop('production_max_steps', None)  # Old parameter name, backward compatibility
        kwargs.pop('production_save_steps', None)
        
        super().__init__(**kwargs)
        
        self.foundation_model = None
        self.foundation_processor = None
        self.foundation_model_name = foundation_model_name

        self.max_iterations = max_iterations
        self.hard_neg_collection_freq = hard_neg_collection_freq
        self.caption_generation_batch_size = caption_generation_batch_size
        
        # Keep track of iterations
        self.current_iteration = 0
        self.iteration_metrics = {}
        
        # Track completion status for resuming
        self._base_training_completed = False
        self._target_embeddings_cached = False
        
        # Save original dataset for reference
        self.original_dataset = self.train_dataset
        
        # Configure logging for train.log generation
        self._configure_logging()
        
        # Try to resume from previous experiment
        self._try_resume_from_checkpoint()
        
        print_master(f"Initialized IterativeRetrievalTrainer with {max_iterations} max iterations")
        
        # Apply fast mode or production mode settings
        self._configure_training_mode()
    
    def _lazy_load_foundation_model(self, to_device: str = None):
        """
        在本 rank 需要生成 caption 时才加载一次 FM；避免训练阶段显存占用。
        """
        if getattr(self, "foundation_model", None) is not None:
            # 已加载；必要时挪到目标设备
            if to_device:
                try:
                    self.foundation_model.to(to_device)
                except Exception:
                    pass
            return self.foundation_model

        if not self.foundation_model_name:
            print_master("No foundation_model_name set; skip loading FM.")
            return None

        import torch
        from transformers import AutoModelForVision2Seq, AutoProcessor
        dev = to_device or (f"cuda:{self.args.process_index}" if torch.cuda.is_available() else "cpu")

        print_master(f"🔁 Lazy-loading foundation model on {dev}: {self.foundation_model_name}")
        fm = AutoModelForVision2Seq.from_pretrained(
            self.foundation_model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            device_map=None
        ).to(dev).eval()

        proc = AutoProcessor.from_pretrained(self.foundation_model_name, trust_remote_code=True)
        setattr(fm, "processor", proc)      # 让 dataset 能通过 foundation_model.processor 访问
        self.foundation_model = fm
        self.foundation_processor = proc
        print_master("✅ Foundation model lazy-loaded.")
        return fm

    def _unload_foundation_model(self):
        """生成结束后立刻卸载，释放显存。"""
        import gc, torch
        try:
            del self.foundation_model
        except Exception:
            pass
        self.foundation_model = None
        try:
            del self.foundation_processor
        except Exception:
            pass
        self.foundation_processor = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print_master("🧹 Foundation model unloaded and CUDA cache cleared.")

    def _configure_logging(self):
        """Configure additional logging to ensure train.log is generated"""
        if hasattr(self.args, 'logging_dir') and self.args.logging_dir:
            # Ensure the logging directory exists
            import os
            os.makedirs(self.args.logging_dir, exist_ok=True)
            
            # Add file handler to logger for train.log
            log_file = os.path.join(self.args.logging_dir, "train.log")
            
            # Check if file handler already exists
            root_logger = logging.getLogger()
            file_handler_exists = any(
                isinstance(handler, logging.FileHandler) and 
                handler.baseFilename == os.path.abspath(log_file)
                for handler in root_logger.handlers
            )
            
            if not file_handler_exists:
                file_handler = logging.FileHandler(log_file, mode='a')
                file_handler.setLevel(logging.INFO)
                formatter = logging.Formatter(
                    '[%(asctime)s] %(levelname)s [%(name)s:%(lineno)s] %(message)s'
                )
                file_handler.setFormatter(formatter)
                root_logger.addHandler(file_handler)
                print_master(f"Added file logging to: {log_file}")
            else:
                print_master(f"File logging already configured for: {log_file}")
        else:
            print_master("Warning: logging_dir not set, train.log will not be generated")
    
    def _configure_training_mode(self):
        """Configure training parameters based on fast mode or production mode"""
        # Debug: Print fast_mode value
        print_master(f"DEBUG: self.fast_mode = {self.fast_mode}")
        print_master(f"DEBUG: steps_per_iteration = {self.steps_per_iteration}")
        
        if self.fast_mode:
            print_master("=== FAST MODE CONFIGURATION ===")
            print_master(f"Steps per iteration: {self.steps_per_iteration}")
            print_master(f"Max samples for hard negatives: {self.fast_mode_max_samples}")
            print_master(f"Retrieval database size: {self.fast_mode_retrieval_db_size}")
            
            # Configure save and logging frequency
            self.args.save_steps = max(1, self.steps_per_iteration // 2)  # Save in the middle
            self.args.logging_steps = 1
            
        else:
            print_master("=== PRODUCTION MODE CONFIGURATION ===")
            print_master(f"Steps per iteration: {self.steps_per_iteration}")
            print_master(f"Save frequency: every {self.production_save_steps} steps")
            
            # Configure save and logging frequency
            self.args.save_steps = self.production_save_steps
            self.args.logging_steps = min(10, self.production_save_steps // 10)
        
        # New strategy: no longer set global max_steps, each iteration decides independently
        # Each iteration will temporarily set self.args.max_steps = self.steps_per_iteration
        print_master("🎯 Each iteration will train independently with fresh optimizer/scheduler")
        
        print_master(f"Final training configuration:")
        print_master(f"  steps_per_iteration: {self.steps_per_iteration}")
        print_master(f"  save_steps: {self.args.save_steps}")
        print_master(f"  logging_steps: {self.args.logging_steps}")
        print_master("=" * 50)
    
    def _try_resume_from_checkpoint(self):
        """Try to resume from a previous checkpoint to avoid recomputation"""
        output_dir = self.args.output_dir
        
        # First, look for the latest COMPLETE iteration state
        latest_complete_iteration = None
        for i in range(self.max_iterations - 1, -1, -1):
            state_file = os.path.join(output_dir, f"iteration_{i}_state.json")
            if os.path.exists(state_file):
                try:
                    with open(state_file, 'r') as f:
                        state = json.load(f)
                    
                    # Check if this iteration is complete
                    if state.get('iteration_complete', False):
                        latest_complete_iteration = i
                        print_master(f"Found COMPLETE iteration {i}")
                        break
                    else:
                        print_master(f"Found INCOMPLETE iteration {i}")
                        # Continue checking for older complete iterations
                except Exception as e:
                    print_master(f"Error reading iteration state {i}: {e}")
                    continue
        
        if latest_complete_iteration is not None:
            # Load metadata from the latest complete iteration
            state_file = os.path.join(output_dir, f"iteration_{latest_complete_iteration}_state.json")
            with open(state_file, 'r') as f:
                state = json.load(f)
            
            self.current_iteration = latest_complete_iteration + 1  # Start from next iteration
            self.iteration_metrics = state.get('iteration_metrics', {})
            
            # Load hard negatives if available and needed
            if latest_complete_iteration < self.max_iterations - 1:
                hard_neg_file = os.path.join(output_dir, f"hard_negatives_iter_{latest_complete_iteration}.json")
                if os.path.exists(hard_neg_file) and hasattr(self.train_dataset, 'hard_negatives_file'):
                    self.train_dataset.hard_negatives_file = hard_neg_file
                    self.train_dataset._load_hard_negatives(latest_complete_iteration)
            
            print_master(f"✅ Resuming from COMPLETE iteration {latest_complete_iteration}")
            print_master(f"   ➡️  Next iteration to run: {self.current_iteration}")
            return True
        
        # If no complete iterations found, check for partial completion
        # Look for incomplete iterations and determine where to resume
        for i in range(self.max_iterations - 1, -1, -1):
            state_file = os.path.join(output_dir, f"iteration_{i}_state.json")
            if os.path.exists(state_file):
                try:
                    with open(state_file, 'r') as f:
                        state = json.load(f)
                    
                    completed_steps = state.get('completed_steps', {})
                    print_master(f"Found INCOMPLETE iteration {i} with steps: {list(completed_steps.keys())}")
                    
                    # Resume from this incomplete iteration
                    self.current_iteration = i
                    self.iteration_metrics = state.get('iteration_metrics', {})
                    
                    print_master(f"🔄 Resuming from INCOMPLETE iteration {i}")
                    print_master(f"   ➡️  Will complete remaining steps in iteration {i}")
                    return True
                    
                except Exception as e:
                    print_master(f"Error reading incomplete iteration state {i}: {e}")
                    continue
        
        # If no iteration state found, check for cached embeddings and base model
        # This handles cases where base training completed but iteration didn't start
        cache_dir = os.path.join(output_dir, "cache")
        base_model_dir = os.path.join(output_dir, "base_model")
        
        # Check for base model completion (most comprehensive check)
        if os.path.exists(base_model_dir):
            print_master(f"Found base model directory: {base_model_dir}")
            
            # Check if base model has required files
            base_model_files = os.listdir(base_model_dir)
            has_adapter = any(f.startswith("adapter_") for f in base_model_files)
            has_config = "adapter_config.json" in base_model_files
            
            if has_adapter and has_config:
                print_master("✅ Base model training appears to be completed (found LoRA adapter)")
                
                # Check for cached embeddings
                if os.path.exists(cache_dir):
                    cache_files = [f for f in os.listdir(cache_dir) if f.startswith("target_embeddings_") and f.endswith(".pt")]
                    if cache_files:
                        print_master(f"✅ Found cached target embeddings: {cache_files}")
                        print_master("🔄 Resuming from completed base training (iteration 0)")
                        print_master("   ➡️  Will skip: base model training, evaluation, target embedding computation")
                        print_master("   ➡️  Will start: hard negative collection")
                        
                        # Set flags to indicate what has been completed
                        self.current_iteration = 0
                        self._base_training_completed = True
                        self._target_embeddings_cached = True
                        return True
                    else:
                        print_master("⚠️  Base model found but no cached embeddings")
                        print_master("🔄 Will resume from base model, recompute embeddings")
                        self.current_iteration = 0
                        self._base_training_completed = True
                        self._target_embeddings_cached = False
                        return True
                else:
                    print_master("⚠️  Base model found but no cache directory")
                    print_master("🔄 Will resume from base model, compute embeddings")
                    self.current_iteration = 0
                    self._base_training_completed = True
                    self._target_embeddings_cached = False
                    return True
        
        # Fallback: check for regular checkpoints without base model
        checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
        if checkpoint_dirs:
            print_master(f"Found checkpoints but no base model: {checkpoint_dirs}")
            print_master("This might be an incomplete training, starting from scratch")
        
        print_master("No previous state, base model, or cached embeddings found, starting from scratch")
        self._base_training_completed = False
        self._target_embeddings_cached = False
        return False
    
    def iterative_train(self, resume_from_iteration: int = 0):
        """
        Main iterative training loop
        
        Args:
            resume_from_iteration (int): Specific iteration to resume from. 
                                       If 0, will auto-detect latest checkpoint.
                                       If > 0, will load from that specific iteration.
        """
        print_master("Starting iterative training process...")
        
        # Resume from specific iteration if specified
        if resume_from_iteration > 0:
            print_master(f"Manually resuming from iteration {resume_from_iteration}")
            self.current_iteration = resume_from_iteration
            self._load_iteration_state(resume_from_iteration - 1)  # Load previous iteration's state
            
            # Also need to load the dataset state for this iteration
            self._prepare_dataset_for_iteration(resume_from_iteration)
        
        for iteration in range(self.current_iteration, self.max_iterations):
            print_master(f"\n{'='*60}")
            print_master(f"Starting Iteration {iteration}")
            print_master(f"{'='*60}")
            
            self.current_iteration = iteration
            
            # Check completion status of all steps for this iteration
            completed_steps = self._check_iteration_completion_status(iteration)
            print_master(f"🔍 Iteration {iteration} completion status: {completed_steps}")
            
            # Step 1: Train current model (or skip if already completed)
            if not completed_steps['model_training']:
                if iteration == 0:
                    print_master("Iteration 0: Training base retrieval model...")
                    self._train_base_model()
                else:
                    print_master(f"Iteration {iteration}: Training with augmented data...")
                    self._train_current_iteration()
                    
                    # Verify training completion status (no longer verify continuity since we reset state)
                    print_master(f"✅ Iteration {iteration} training completed with fresh optimizer/scheduler")
                
                # Add sync barrier: ensure all GPUs complete training
                if dist.is_initialized():
                    dist.barrier()
                    print_master(f"All GPUs completed training for iteration {iteration}")
            else:
                print_master("✅ Model training already completed, skipping...")
            
            # Step 2: Evaluate current model performance (or skip if cached)
            if not completed_steps['evaluation']:
                # Add distributed barrier to ensure all GPUs complete training before evaluation
                if dist.is_initialized():
                    dist.barrier()
                
                # Use the improved evaluation method that handles distributed internally
                eval_results = self._evaluate_current_model()
                self.iteration_metrics[iteration] = eval_results
            else:
                print_master("✅ Model evaluation already completed, loading cached results...")
                # Load cached evaluation results
                eval_file = os.path.join(self.args.output_dir, f"eval_results_iter_{iteration}.json")
                with open(eval_file, 'r') as f:
                    eval_results = json.load(f)
                self.iteration_metrics[iteration] = eval_results
            
            # Step 3: Collect hard negatives (if not last iteration)
            if iteration < self.max_iterations - 1:
                if not completed_steps['hard_negatives_collection']:
                    print_master(f"🔍 Starting hard negative collection for iteration {iteration}...")
                    hard_neg_start_time = time.time()
                    
                    hard_negatives = self._collect_hard_negatives(iteration)
                    
                    hard_neg_time = time.time() - hard_neg_start_time
                    print_master(f"Hard negative collection completed in {int(hard_neg_time//60):02d}:{int(hard_neg_time%60):02d}")
                    
                    # Add sync barrier: ensure all GPUs complete hard negative collection
                    if dist.is_initialized():
                        dist.barrier()
                        print_master(f"All GPUs completed hard negative collection for iteration {iteration}")
                else:
                    print_master("✅ Hard negative collection already completed, loading cached results...")
                    # Load cached hard negatives
                    hard_neg_file = os.path.join(self.args.output_dir, f"hard_negatives_iter_{iteration}.json")
                    with open(hard_neg_file, 'r') as f:
                        hard_negatives = json.load(f)
                    hard_neg_time = 0  # No time spent since cached
                
                # Step 4: Generate augmented captions using foundation model
                if len(hard_negatives) > 0:
                    if not completed_steps['caption_generation']:
                        print_master(f"📝 Starting caption generation for {len(hard_negatives)} hard negatives...")
                        caption_start_time = time.time()
                        
                        augmented_samples = self._generate_augmented_captions(hard_negatives)
                        
                        caption_time = time.time() - caption_start_time
                        print_master(f"Caption generation completed in {int(caption_time//60):02d}:{int(caption_time%60):02d}")
                        
                        # Add sync barrier: ensure all GPUs complete caption generation
                        if dist.is_initialized():
                            dist.barrier()
                            print_master(f"All GPUs completed caption generation for iteration {iteration}")
                        
                        # Step 5: Prepare dataset for next iteration
                        self._prepare_next_iteration_dataset(iteration + 1, augmented_samples)
                    else:
                        print_master("✅ Caption generation already completed, loading cached results...")
                        # Load cached augmented samples
                        augmented_file = os.path.join(self.args.output_dir, f"augmented_samples_iter_{iteration + 1}.json")
                        with open(augmented_file, 'r') as f:
                            saved_data = json.load(f)
                        augmented_samples = saved_data.get('samples', [])
                        caption_time = 0  # No time spent since cached
                    
                    # Performance statistics
                    total_time = hard_neg_time + caption_time
                    print_master(f"📊 Iteration {iteration} data preparation stats:")
                    print_master(f"  - Hard negatives: {len(hard_negatives)} samples in {hard_neg_time:.1f}s")
                    print_master(f"  - Augmented captions: {len(augmented_samples)} samples in {caption_time:.1f}s")
                    print_master(f"  - Total time: {int(total_time//60):02d}:{int(total_time%60):02d}")
                    
                    if dist.is_initialized():
                        world_size = dist.get_world_size()
                        print_master(f"  - Used {world_size} GPUs for parallel processing")
                        if total_time > 0:
                            print_master(f"  - Processing rate: {(len(hard_negatives) + len(augmented_samples))/total_time:.2f} samples/second")
                else:
                    print_master("No hard negatives found, stopping early")
                    break
            
            # CRITICAL FIX: Sync barrier ensure all GPUs complete current iteration before saving state
            if dist.is_initialized():
                dist.barrier()
                print_master(f"All GPUs completed iteration {iteration}, saving state...")
            
            # Save iteration checkpoint (only rank 0 writes)
            self._save_iteration_state(iteration)
        
        print_master("\nIterative training completed!")
        self._summarize_results()
    
    def _train_base_model(self):
        """Train the base retrieval model using standard contrastive learning"""
        print_master("Training base model with original dataset and fresh optimizer...")
        
        # Use original dataset
        self.train_dataset = self.original_dataset
        self._update_train_dataloader()
        
        # Create independent subdirectory for base model training to avoid checkpoint conflicts
        original_output_dir = self.args.output_dir      # Save main output directory
        original_max_steps = self.args.max_steps        # Save original value
        base_training_dir = os.path.join(original_output_dir, "training_iter_0")
        
        # Create base model specific training directory
        os.makedirs(base_training_dir, exist_ok=True)
        self.args.output_dir = base_training_dir         # Temporarily change to subdirectory
        self.args.max_steps = self.steps_per_iteration   # Set for current iteration steps
        
        print_master(f"🎯 Base model training plan: 0 → {self.args.max_steps} steps")
        print_master(f"🆕 Starting fresh training with new optimizer and scheduler")
        print_master(f"📁 Training checkpoints will be saved to: {base_training_dir}")
        print_master(f"📁 Final base model will be saved to: {original_output_dir}")

        try:
            # resume_from_checkpoint=None ensures new optimizer and scheduler creation
            train_result = self.train(resume_from_checkpoint=None)
        finally:
            # Restore original output directory and max_steps
            self.args.output_dir = original_output_dir
            self.args.max_steps = original_max_steps
            print_master(f"✅ Restored output_dir to: {original_output_dir}")
            print_master(f"✅ Restored max_steps to: {original_max_steps}")
        
        # Save base model to main directory (not subdirectory)
        base_model_path = os.path.join(original_output_dir, "base_model")
        self.save_model(base_model_path)
        
        print_master(f"Base model training completed: 0 → {self.state.global_step} steps")
        print_master(f"✅ Base model saved to: {base_model_path}")
        
        return train_result
    
    def _train_current_iteration(self):
        """
        Train for the current iteration by loading previous model weights 
        but resetting the optimizer and LR scheduler for independent training.
        """
        print_master(f"Training iteration {self.current_iteration} with RESET optimizer and scheduler...")
        
        # 1. Important note: model weights already loaded correctly in main() function
        # trainer only handles training flow control, separation of responsibilities architecture
        print_master("🧠 Model weights already loaded by main() function")
        print_master("🔄 Will reset optimizer and scheduler for independent learning rate schedule")
        
        # 2. Ensure dataset updated to current iteration state
        self._update_train_dataloader()
        
        # 3. Key: create independent subdirectory for each iteration training to avoid checkpoint conflicts
        original_output_dir = self.args.output_dir  # Save main output directory
        original_max_steps = self.args.max_steps    # Save original max_steps
        iteration_output_dir = os.path.join(original_output_dir, f"training_iter_{self.current_iteration}")
        
        # Create iteration specific training directory
        os.makedirs(iteration_output_dir, exist_ok=True)
        self.args.output_dir = iteration_output_dir        # Temporarily change to subdirectory
        self.args.max_steps = self.steps_per_iteration     # Set independent steps for this iteration
        
        print_master(f"🎯 Iteration {self.current_iteration} independent training plan:")
        print_master(f"   - Will train for {self.args.max_steps} fresh steps")
        print_master(f"   - Training checkpoints will be saved to: {iteration_output_dir}")
        print_master(f"   - Final iteration model will be saved to: {original_output_dir}")
        print_master(f"   - Previous global_step will be ignored for LR scheduling")
        print_master(f"   - New optimizer and scheduler will start from scratch")

        try:
            # 4. Important note: model weights already loaded in main() function
            # trainer only handles training flow control, no longer handle model weight loading
            print_master("✅ Model weights already loaded in main(), ready for independent training")
            print_master("🔄 Creating fresh optimizer and scheduler for this iteration")
            
            # 5. Key: call train without resume_from_checkpoint
            # This forces Trainer to create fresh optimizer and learning rate scheduler
            # New scheduler will complete entire learning rate decay within self.steps_per_iteration steps
            train_result = self.train(resume_from_checkpoint=None)
            
        finally:
            # Key: restore original output directory and max_steps
            self.args.output_dir = original_output_dir
            self.args.max_steps = original_max_steps
            print_master(f"✅ Restored output_dir to: {original_output_dir}")
            print_master(f"✅ Restored max_steps to: {original_max_steps}")
        
        print_master(f"Iteration {self.current_iteration} independent training completed")
        print_master(f"Final step count: {self.state.global_step}")
        
        # Save final iteration model to main directory (not subdirectory)
        iter_model_path = os.path.join(original_output_dir, f"iteration_{self.current_iteration}")
        self.save_model(iter_model_path)
        print_master(f"✅ Final iteration model saved to: {iter_model_path}")
        
        return train_result
    
    def _update_train_dataloader(self):
        """Update train dataloader to reflect dataset changes"""
        if hasattr(self, '_train_dataloader'):
            # Clear cached dataloader to force recreation with new dataset
            del self._train_dataloader
        
        # Force recreation of dataloader with updated dataset
        self.get_train_dataloader()
        
        # Get detailed dataset information for more accurate logging
        total_samples = len(self.train_dataset)
        if hasattr(self.train_dataset, 'augmented_samples'):
            augmented_count = len(self.train_dataset.augmented_samples)
            original_count = total_samples - augmented_count
            print_master(f"🔄 Updated train dataloader:")
            print_master(f"  - Total samples: {total_samples}")
            print_master(f"  - Original samples: {original_count}")
            print_master(f"  - Augmented samples: {augmented_count}")
        else:
            print_master(f"🔄 Updated train dataloader with {total_samples} total samples")
    
    def _evaluate_current_model(self) -> Dict[str, float]:
        """Evaluate current model on validation set with optimizations and caching"""
        print_master(f"Evaluating iteration {self.current_iteration} model...")
        
        # Check if evaluation results already exist for this iteration
        eval_results_file = os.path.join(self.args.output_dir, f"eval_results_iter_{self.current_iteration}.json")
        if os.path.exists(eval_results_file):
            print_master(f"Found cached evaluation results for iteration {self.current_iteration}, loading...")
            try:
                with open(eval_results_file, 'r') as f:
                    cached_results = json.load(f)
                print_master(f"Loaded cached evaluation results: {cached_results}")
                return cached_results
            except Exception as e:
                print_master(f"Error loading cached evaluation results: {e}, proceeding with fresh evaluation")
        
        try:
            # Initialize evaluator if not already done
            if not hasattr(self, 'evaluator') or self.evaluator is None:
                from .evaluation.cirr_evaluator import CIRREvaluator
                
                # Adjust batch size based on fast mode
                eval_batch_size = 4 if self.fast_mode else 8
                
                self.evaluator = CIRREvaluator(
                    model=self.model,
                    processor=self.processing_class,
                    data_args=self.data_args,
                    model_args=self.model_args,
                    device=self.args.device,
                    batch_size=eval_batch_size
                )
                print_master(f"Real evaluator initialized successfully (batch_size={eval_batch_size})")
            
            # Check if distributed evaluation is available and beneficial
            use_distributed = (dist.is_initialized() and 
                             dist.get_world_size() > 1 and 
                             hasattr(self.evaluator, '_evaluate_distributed'))
            
            # In fast mode, prefer single GPU evaluation for simplicity unless many GPUs
            # Modified: Allow distributed evaluation with 2+ GPUs even in fast mode
            if self.fast_mode and use_distributed and dist.get_world_size() < 2:
                use_distributed = False
                print_master("Fast mode: Using single GPU evaluation for single GPU setup")
            
            if use_distributed:
                print_master(f"Using distributed evaluation across {dist.get_world_size()} GPUs")
                eval_results = self.evaluator.evaluate(distributed=True)
            else:
                print_master("Using single GPU evaluation")
                eval_results = self.evaluator.evaluate(distributed=False)
            
            # Add evaluation metadata
            eval_results['evaluation_mode'] = 'distributed' if use_distributed else 'single_gpu'
            eval_results['fast_mode'] = self.fast_mode
            eval_results['iteration'] = self.current_iteration
            
        except Exception as e:
            print_master(f"Real evaluation failed: {e}")
            print_master("Falling back to dummy evaluation")
            # Fallback to dummy metrics with realistic fast mode values
            if self.fast_mode:
                # Fast mode typically has lower performance due to limited training
                eval_results = {
                    'recall_at_1': 0.15,  
                    'recall_at_5': 0.35,  
                    'recall_at_10': 0.45,  
                    'recall_subset_at_1': 0.12,
                    'recall_subset_at_2': 0.25,
                    'recall_subset_at_3': 0.32,
                    'group_recall_at_1': 0.18,
                    'group_recall_at_2': 0.30,
                    'group_recall_at_3': 0.38,
                    'evaluation_mode': 'dummy_fast',
                    'fast_mode': True,
                    'iteration': self.current_iteration
                }
            else:
                # Production mode dummy metrics
                eval_results = {
                    'recall_at_1': 0.5,  
                    'recall_at_5': 0.7,  
                    'recall_at_10': 0.8,  
                    'recall_subset_at_1': 0.3,
                    'recall_subset_at_2': 0.5,
                    'recall_subset_at_3': 0.6,
                    'group_recall_at_1': 0.4,
                    'group_recall_at_2': 0.6,
                    'group_recall_at_3': 0.7,
                    'evaluation_mode': 'dummy_production',
                    'fast_mode': False,
                    'iteration': self.current_iteration
                }
        
        # Save evaluation results to cache for future use (only main process writes)
        if not dist.is_initialized() or dist.get_rank() == 0:
            eval_results_file = os.path.join(self.args.output_dir, f"eval_results_iter_{self.current_iteration}.json")
            try:
                with open(eval_results_file, 'w') as f:
                    json.dump(eval_results, f, indent=2)
                print_master(f"Saved evaluation results to {eval_results_file}")
            except Exception as e:
                print_master(f"Warning: Failed to save evaluation results: {e}")
        
        print_master(f"Iteration {self.current_iteration} results: {eval_results}")
        return eval_results
    
    def _collect_hard_negatives(self, iteration: int) -> List[Dict]:
        """Collect hard negatives using current model with caching support"""
        print_master(f"Collecting hard negatives for iteration {iteration}...")
        
        # Check if hard negatives already exist for this iteration
        cache_file = os.path.join(self.args.output_dir, f"hard_negatives_iter_{iteration}.json")
        if os.path.exists(cache_file):
            print_master(f"Found cached hard negatives for iteration {iteration}, loading...")
            with open(cache_file, 'r') as f:
                cached_data = json.load(f)
            print_master(f"Loaded {len(cached_data)} cached hard negatives")
            return cached_data
        
        # Set model to evaluation mode
        self.model.eval()
        
        if isinstance(self.train_dataset, (IterativeCIRRDataset, IterativeFashionIQDataset)):
            # Determine sample limit based on mode
            sample_limit = self.fast_mode_max_samples if self.fast_mode else None
            
            # Also pass retrieval database size information to the dataset
            if self.fast_mode and hasattr(self.train_dataset, 'fast_mode_retrieval_db_size'):
                self.train_dataset.fast_mode_retrieval_db_size = self.fast_mode_retrieval_db_size
            
            # Use distributed hard negative collection
            if dist.is_initialized() and dist.get_world_size() > 1:
                print_master("Using distributed hard negative collection...")
                hard_negatives = self.train_dataset.collect_hard_negatives_batch_distributed(
                    self.model,
                    batch_size=8,  # Fixed batch size for faster processing
                    max_samples=sample_limit  # Pass limit to dataset
                )
            else:
                print_master("Using single-GPU hard negative collection...")
                hard_negatives = self.train_dataset.collect_hard_negatives_batch(
                    self.model,
                    batch_size=8,  # Fixed batch size for faster processing
                    max_samples=sample_limit  # Pass limit to dataset
                )
            
            print_master(f"Collected {len(hard_negatives)} hard negatives " + 
                        (f"(limited to {sample_limit})" if sample_limit else "(no limit)"))
            
            # Cache the results (only master process)
            if not dist.is_initialized() or dist.get_rank() == 0:
                os.makedirs(os.path.dirname(cache_file), exist_ok=True)
                with open(cache_file, 'w') as f:
                    json.dump(hard_negatives, f, indent=2)
                print_master(f"Cached hard negatives to {cache_file}")
            
        else:
            # Fallback: implement hard negative collection here
            hard_negatives = self._collect_hard_negatives_fallback()
        
        print_master(f"Collected {len(hard_negatives)} hard negatives")
        return hard_negatives
    
    def _collect_hard_negatives_fallback(self) -> List[Dict]:
        """Fallback method for hard negative collection with fast mode support"""
        # This should implement retrieval evaluation and hard negative identification
        print_master("Using fallback hard negative collection")
        
        # Generate dummy hard negatives for testing
        dummy_count = self.fast_mode_max_samples if self.fast_mode else 500
        dummy_negatives = []
        for i in range(dummy_count):
            dummy_negatives.append({
                'query_id': f'dummy_query_{i}',
                'reference_image': f'dummy_ref_{i}',
                'target_image': f'dummy_target_{i}',
                'original_caption': f'dummy caption {i}',
                'difficulty_score': 0.8 + (i % 5) * 0.04  # Simulate difficulty scores
            })
        
        print_master(f"Generated {len(dummy_negatives)} dummy hard negatives")
        return dummy_negatives
    
    def _generate_augmented_captions(self, hard_negatives: List[Dict]) -> List[Dict]:
        """Generate augmented captions using foundation model (lazy load & unload)."""
        import torch, torch.distributed as dist

        dev = f"cuda:{self.args.process_index}" if torch.cuda.is_available() else "cpu"
        fm = self._lazy_load_foundation_model(to_device=dev)
        if fm is None:
            print_master("No foundation model available, skip caption generation")
            return []

        print_master(f"Generating augmented captions for {len(hard_negatives)} samples...")
        augmented_samples: List[Dict] = []

        # 仅支持这两类迭代数据集
        if not isinstance(self.train_dataset, (IterativeCIRRDataset, IterativeFashionIQDataset)):
            print_master("Train dataset is not iterative CIRR/FashionIQ, skip caption generation")
            self._unload_foundation_model()
            return augmented_samples

        # 把 FM 和必要配置挂到 dataset
        if hasattr(self.train_dataset, "foundation_model"):
            self.train_dataset.foundation_model = fm
        # 将 caption 批大小传给 dataset（若其方法内会使用）
        if hasattr(self, "caption_generation_batch_size"):
            setattr(self.train_dataset, "caption_generation_batch_size", self.caption_generation_batch_size)

        # 缺省指定 backbone 一次（仅当缺失时）
        if hasattr(self.train_dataset, "model_args") and not getattr(self.train_dataset.model_args, "foundation_model_backbone", None):
            setattr(self.train_dataset.model_args, "foundation_model_backbone", "qwen2_vl")

        try:
            with torch.inference_mode():
                # 分布式优先走分布式接口
                if dist.is_initialized() and dist.get_world_size() > 1 and hasattr(self.train_dataset, "generate_augmented_captions_distributed"):
                    print_master("Using distributed caption generation...")
                    augmented_samples = self.train_dataset.generate_augmented_captions_distributed(hard_negatives)
                else:
                    print_master("Using single-GPU caption generation...")
                    augmented_samples = self.train_dataset.generate_augmented_captions(hard_negatives)
        except Exception as e:
            print_master(f"Caption generation failed: {e}")
            augmented_samples = []
        finally:
            # 用完立刻卸载，释放显存，并清理 dataset 引用避免悬挂
            self._unload_foundation_model()
            if hasattr(self.train_dataset, "foundation_model"):
                self.train_dataset.foundation_model = None

        print_master(f"Generated {len(augmented_samples)} augmented samples")
        return augmented_samples
    
    def _generate_captions_fallback(self, hard_negatives: List[Dict]) -> List[Dict]:
        """Fallback method for caption generation"""
        # This should implement foundation model caption generation
        # For now, return empty list
        print_master("Using fallback caption generation")
        return []
    
    def _prepare_next_iteration_dataset(self, next_iteration: int, augmented_samples: List[Dict]):
        """Prepare dataset for next iteration with augmented samples"""
        print_master(f"Preparing dataset for iteration {next_iteration}...")
        
        # Save augmented samples to file for resuming with metadata
        augmented_file = os.path.join(self.args.output_dir, f"augmented_samples_iter_{next_iteration}.json")
        
        # Create metadata structure
        augmented_data = {
            "total_samples": len(augmented_samples),
            "generation_timestamp": __import__('time').time(),
            "iteration_round": next_iteration,
            "sample_statistics": self._compute_sample_statistics(augmented_samples),
            "samples": augmented_samples
        }
        
        # Save augmented samples to file (only main process writes)
        if not dist.is_initialized() or dist.get_rank() == 0:
            with open(augmented_file, 'w') as f:
                json.dump(augmented_data, f, indent=2)
            print_master(f"Saved {len(augmented_samples)} augmented samples to {augmented_file}")
        else:
            print_rank(f"GPU {dist.get_rank()}: Skipping augmented samples save (only rank 0 writes)")
        
        # Sync barrier: ensure all GPUs can see the saved file
        if dist.is_initialized():
            dist.barrier()
            print_master("All GPUs synchronized after augmented samples save")
        
        if isinstance(self.train_dataset, (IterativeCIRRDataset, IterativeFashionIQDataset)):
            # Record dataset state before update
            old_dataset_len = len(self.train_dataset)
            old_augmented_len = len(self.train_dataset.augmented_samples)
            
            # Update dataset with augmented samples
            self.train_dataset.iteration_round = next_iteration
            self.train_dataset.augmented_samples.extend(augmented_samples)
            
            # Record dataset state after update
            new_dataset_len = len(self.train_dataset)
            new_augmented_len = len(self.train_dataset.augmented_samples)
            
            # Update hard negatives file path for new iteration
            self.train_dataset.hard_negatives_file = os.path.join(
                self.args.output_dir, f"hard_negatives_iter_{next_iteration}.json"
            )
            
            # More accurate logging
            print_master(f"📊 Dataset update summary:")
            print_master(f"  - Added {len(augmented_samples)} new augmented samples")
            print_master(f"  - Total augmented samples: {old_augmented_len} → {new_augmented_len}")
            print_master(f"  - Total dataset size: {old_dataset_len} → {new_dataset_len}")
            
        else:
            # Create new iterative dataset
            dataset_config = {
                'dataset_name': 'composed_retrieval',# Save augmented samp
                'iteration_round': next_iteration
            }
            
            # Determine dataset type based on current dataset
            if 'cirr' in str(self.train_dataset).lower():
                self.train_dataset = IterativeCIRRDataset(
                    model_args=self.model_args,
                    data_args=self.data_args,
                    training_args=self.args,
                    iteration_round=next_iteration,
                    foundation_model=self.foundation_model,
                    experiment_dir=self.args.output_dir,  # Pass experiment directory
                    **dataset_config
                )
            else:
                self.train_dataset = IterativeFashionIQDataset(
                    model_args=self.model_args,
                    data_args=self.data_args,
                    training_args=self.args,
                    iteration_round=next_iteration,
                    foundation_model=self.foundation_model,
                    experiment_dir=self.args.output_dir,  # Pass experiment directory
                    **dataset_config
                )
        
        # Important: force update trainer's dataloader to reflect dataset changes
        self._update_train_dataloader()
        print_master(f"Training dataloader updated for iteration {next_iteration}")
    
    def _compute_sample_statistics(self, samples: List[Dict]) -> Dict[str, Any]:
        """Compute statistics for augmented samples"""
        if not samples:
            return {}
        
        try:
            # Compute text length statistics
            original_lengths = []
            generated_lengths = []
            reference_images = set()
            target_images = set()
            
            for sample in samples:
                # Original text length
                if 'original_mod_text' in sample:
                    original_lengths.append(len(sample['original_mod_text']))
                
                # Generated text length
                if 'modification_text' in sample:
                    generated_lengths.append(len(sample['modification_text']))
                
                # Unique images
                if 'reference_image' in sample:
                    reference_images.add(sample['reference_image'])
                if 'target_image' in sample:
                    target_images.add(sample['target_image'])
            
            statistics = {
                'total_samples': len(samples),
                'avg_original_length': sum(original_lengths) / len(original_lengths) if original_lengths else 0,
                'avg_generated_length': sum(generated_lengths) / len(generated_lengths) if generated_lengths else 0,
                'unique_reference_images': len(reference_images),
                'unique_target_images': len(target_images)
            }
            
            # Add augmented sample ratio if available
            augmented_count = sum(1 for s in samples if s.get('is_augmented', False))
            statistics['augmented_ratio'] = augmented_count / len(samples) if samples else 0
            
            return statistics
            
        except Exception as e:
            print_master(f"Warning: Failed to compute sample statistics: {e}")
            return {'total_samples': len(samples)}
    
    def _prepare_dataset_for_iteration(self, iteration: int):
        """Prepare dataset state for a specific iteration when resuming"""
        print_master(f"Preparing dataset for resumed iteration {iteration}...")
        
        if iteration == 0:
            # Use original dataset for iteration 0
            self.train_dataset = self.original_dataset
            return
        
        # For iterations > 0, need to load accumulated augmented samples
        all_augmented_samples = []
        
        # Load augmented samples from all previous iterations
        for i in range(1, iteration+1):
            augmented_file = os.path.join(self.args.output_dir, f"augmented_samples_iter_{i}.json")
            if os.path.exists(augmented_file):
                with open(augmented_file, 'r') as f:
                    data = json.load(f)
                
                # Extract actual samples from the data structure
                if isinstance(data, dict) and 'samples' in data:
                    # New format with metadata
                    iter_samples = data['samples']
                    print_master(f"Loaded {len(iter_samples)} augmented samples from iteration {i} (with metadata)")
                elif isinstance(data, list):
                    # Old format - direct list of samples
                    iter_samples = data
                    print_master(f"Loaded {len(iter_samples)} augmented samples from iteration {i} (direct list)")
                else:
                    print_master(f"Warning: Unexpected data format in {augmented_file}, skipping...")
                    continue
                
                all_augmented_samples.extend(iter_samples)
        
        # Update dataset with all accumulated samples
        if isinstance(self.train_dataset, (IterativeCIRRDataset, IterativeFashionIQDataset)):
            self.train_dataset.iteration_round = iteration
            self.train_dataset.augmented_samples = all_augmented_samples
            # Set hard negatives file for current iteration
            self.train_dataset.hard_negatives_file = os.path.join(
                self.args.output_dir, f"hard_negatives_iter_{iteration-1}.json"
            )
        
        print_master(f"Dataset prepared for iteration {iteration} with {len(all_augmented_samples)} total augmented samples")
    
    def _save_iteration_state(self, iteration: int):
        """Save iteration state and metrics with step completion tracking"""
        # CRITICAL FIX: only main process (rank 0) writes state file to avoid race conditions
        if dist.is_initialized() and dist.get_rank() != 0:
            print_rank(f"GPU {dist.get_rank()}: Skipping state save (only rank 0 writes)")
            return
        
        state_file = os.path.join(self.args.output_dir, f"iteration_{iteration}_state.json")
        
        # Determine correct model path based on iteration
        if iteration == 0:
            model_path = os.path.join(self.args.output_dir, "base_model")
        else:
            model_path = os.path.join(self.args.output_dir, f"iteration_{iteration}")
        
        # Check completion status of all iteration steps
        completed_steps = self._check_iteration_completion_status(iteration)
        
        state = {
            'iteration': iteration,
            'metrics': self.iteration_metrics,
            'model_path': model_path,
            'hard_negatives_file': f"hard_negatives_iter_{iteration}.json",
            'completed_steps': completed_steps,
            'iteration_complete': completed_steps.get('all_steps_complete', False),
            'timestamp': time.time()
        }
        
        try:
            with open(state_file, 'w') as f:
                json.dump(state, f, indent=2)
            
            completion_status = "✅ COMPLETE" if state['iteration_complete'] else "🔄 IN PROGRESS"
            print_master(f"Saved iteration {iteration} state to {state_file} - {completion_status}")
            print_master(f"Model path recorded as: {model_path}")
            print_master(f"Completed steps: {list(completed_steps.keys())}")
        except Exception as e:
            print_master(f"❌ Error saving iteration state: {e}")
    
    def _check_iteration_completion_status(self, iteration: int) -> dict:
        """Check which steps of an iteration have been completed"""
        output_dir = self.args.output_dir
        completed_steps = {}
        
        # Step 1: Model training completed
        if iteration == 0:
            model_path = os.path.join(output_dir, "base_model")
        else:
            model_path = os.path.join(output_dir, f"iteration_{iteration}")
        
        completed_steps['model_training'] = os.path.exists(model_path)
        
        # Step 2: Evaluation completed
        eval_file = os.path.join(output_dir, f"eval_results_iter_{iteration}.json")
        completed_steps['evaluation'] = os.path.exists(eval_file)
        
        # Step 3: Hard negatives collected (only for non-final iterations)
        is_final_iteration = iteration >= (self.max_iterations - 1)
        if is_final_iteration:
            completed_steps['hard_negatives_collection'] = True  # Not needed for final iteration
        else:
            hard_neg_file = os.path.join(output_dir, f"hard_negatives_iter_{iteration}.json")
            completed_steps['hard_negatives_collection'] = os.path.exists(hard_neg_file)
        
        # Step 4: Caption generation completed (only for non-final iterations)
        if is_final_iteration:
            completed_steps['caption_generation'] = True  # Not needed for final iteration
        else:
            next_iteration = iteration + 1
            augmented_file = os.path.join(output_dir, f"augmented_samples_iter_{next_iteration}.json")
            completed_steps['caption_generation'] = os.path.exists(augmented_file)
        
        # Check if all required steps are complete
        completed_steps['all_steps_complete'] = all(completed_steps.values())
        
        return completed_steps
    
    def _load_iteration_state(self, iteration: int):
        """Load iteration state for resuming (model already loaded externally)"""
        state_file = os.path.join(self.args.output_dir, f"iteration_{iteration}_state.json")
        
        if os.path.exists(state_file):
            with open(state_file, 'r') as f:
                state = json.load(f)
            
            # Note: Model weights already loaded in main script using MMEBModel.load()
            print_master(f"Loading iteration {iteration} metadata (model loaded separately)")
            
            self.iteration_metrics = state.get('iteration_metrics', {})
            print_master(f"Loaded iteration {iteration} state from {state_file}")
        else:
            print_master(f"No state file found for iteration {iteration}")
    
    def _summarize_results(self):
        """Summarize results across all iterations"""
        print_master("\n" + "="*80)
        print_master("ITERATIVE TRAINING SUMMARY")
        print_master("="*80)
        
        for iteration, metrics in self.iteration_metrics.items():
            print_master(f"Iteration {iteration}: {metrics}")
        
        # Find best iteration
        if self.iteration_metrics:
            def _score(m):  # 兼容不同命名
               return m.get('recall_at_1', m.get('r_at_1', 0))
            best_iteration = max(self.iteration_metrics.keys(), key=lambda x: _score(self.iteration_metrics[x]))
            best_metrics = self.iteration_metrics[best_iteration]
            
            print_master(f"\nBest performance: Iteration {best_iteration}")
            print_master(f"Best metrics: {best_metrics}")
        
        # Save summary (only main process writes)
        if not dist.is_initialized() or dist.get_rank() == 0:
            summary_file = os.path.join(self.args.output_dir, "training_summary.json")
            summary = {
                'max_iterations': self.max_iterations,
                'completed_iterations': len(self.iteration_metrics),
                'iteration_metrics': self.iteration_metrics,
                'best_iteration': best_iteration if self.iteration_metrics else None,
                'best_metrics': best_metrics if self.iteration_metrics else None
            }
            
            with open(summary_file, 'w') as f:
                json.dump(summary, f, indent=2)
            
            print_master(f"Training summary saved to {summary_file}")


def create_iterative_trainer(
    model: MMEBModel,
    foundation_model=None,
    args: TrainingArguments = None,
    train_dataset=None,
    eval_dataset=None,
    experiment_dir=None,
    **kwargs
) -> IterativeRetrievalTrainer:
    """
    Factory function to create iterative trainer
    """
    # Extract iterative-specific parameters
    iterative_params = {}
    for key in ['max_iterations', 'hard_neg_collection_freq', 'caption_generation_batch_size']:
        if key in kwargs:
            iterative_params[key] = kwargs.pop(key)
    
    # Extract fast mode and production mode parameters
    fast_mode_params = {}
    for key in ['fast_mode', 'fast_mode_max_samples', 'fast_mode_retrieval_db_size', 
                'fast_mode_max_steps', 'steps_per_iteration', 'production_save_steps',
                'production_max_steps']:  # Keep backward compatibility
        if key in kwargs:
            fast_mode_params[key] = kwargs.pop(key)
    
    # Debug: Print extracted parameters
    print_master(f"DEBUG: Extracted fast_mode_params = {fast_mode_params}")
    
    # Don't pass experiment_dir to base trainer
    if experiment_dir:
        # We can use args.output_dir instead
        pass
    
    # Extract standard trainer parameters
    trainer_params = {}
    standard_trainer_args = [
        'data_collator', 'tokenizer', 'model_init', 'compute_metrics',
        'callbacks', 'optimizers', 'preprocess_logits_for_metrics',
        'processing_class'
    ]
    
    for key in standard_trainer_args:
        if key in kwargs:
            trainer_params[key] = kwargs.pop(key)
    
    # Extract important args that trainer needs
    important_args = {}
    for key in ['model_args', 'data_args', 'max_length']:
        if key in kwargs:
            important_args[key] = kwargs.pop(key)
    
    # NEW: pull foundation_model_name from kwargs (if provided)
    foundation_model_name = kwargs.pop('foundation_model_name', None)

    # Remaining kwargs are ignored
    
    return IterativeRetrievalTrainer(
        model=model,
        foundation_model=foundation_model,
        foundation_model_name=foundation_model_name,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        **iterative_params,
        **fast_mode_params,
        **trainer_params,
        **important_args
    )
