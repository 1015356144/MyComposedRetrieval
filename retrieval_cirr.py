#!/usr/bin/env python3
"""
CIRRæ£€ç´¢è„šæœ¬ - åŠ è½½æ¨¡å‹å¯¹CIRRéªŒè¯é›†è¿›è¡Œæ£€ç´¢å¹¶ä¿å­˜top-kç»“æœ
åŸºäºç°æœ‰è¯„ä¼°ä»£ç æ”¹ç¼–ï¼Œä¸“é—¨ç”¨äºæ£€ç´¢å’Œä¿å­˜ç»“æœ
"""

import os
import sys
import json
import torch
import logging
import traceback
from datetime import datetime
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, field
from transformers import HfArgumentParser
import torch.distributed as dist
import torch.nn.functional as F
from tqdm import tqdm
from PIL import Image

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.arguments import ModelArguments, DataArguments
from src.model.model import MMEBModel
from src.model.processor import load_processor, get_backbone_name, VLM_IMAGE_TOKENS
from src.utils import print_rank, print_master

logging.basicConfig(
    level=logging.INFO, 
    format='[%(asctime)s] %(levelname)s [%(name)s:%(lineno)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


@dataclass
class CIRRRetrievalArguments:
    """CIRRæ£€ç´¢ä¸“ç”¨å‚æ•°"""
    model_path: str = field(
        metadata={"help": "è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (å¯ä»¥æ˜¯checkpoint-xxxæˆ–iteration_xç›®å½•)"}
    )
    base_model_name: str = field(
        default=None,
        metadata={"help": "åŸºç¡€æ¨¡å‹åç§° (ä¾‹å¦‚ Qwen/Qwen2-VL-2B-Instruct). å¦‚æœä¸æä¾›ï¼Œå°†å°è¯•ä»model_pathæ¨æ–­"}
    )
    output_file: str = field(
        default=None,
        metadata={"help": "ä¿å­˜æ£€ç´¢ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„. å¦‚æœä¸æä¾›ï¼Œå°†è‡ªåŠ¨ç”ŸæˆåŸºäºæ¨¡å‹è·¯å¾„çš„æ–‡ä»¶å"}
    )
    top_k: int = field(
        default=10,
        metadata={"help": "ä¿å­˜æ¯ä¸ªæŸ¥è¯¢çš„top-kæ£€ç´¢ç»“æœ (é»˜è®¤: 10)"}
    )
    batch_size: int = field(
        default=8,
        metadata={"help": "æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8)"}
    )
    device: str = field(
        default="auto",
        metadata={"help": "ä½¿ç”¨çš„è®¾å¤‡: 'auto', 'cuda', 'cuda:0', ç­‰"}
    )
    cirr_data_dir: str = field(
        default=None,
        metadata={"help": "CIRRæ•°æ®é›†ç›®å½•è·¯å¾„"}
    )
    cirr_image_dir: str = field(
        default=None,
        metadata={"help": "CIRRå›¾åƒç›®å½•è·¯å¾„"}
    )
    save_embeddings: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦åŒæ—¶ä¿å­˜æŸ¥è¯¢å’Œå€™é€‰å›¾åƒçš„åµŒå…¥å‘é‡"}
    )


class CIRRRetriever:
    """
    CIRRæ£€ç´¢å™¨ - åŸºäºCIRREvaluatoræ”¹ç¼–ï¼Œä¸“é—¨ç”¨äºæ£€ç´¢å’Œä¿å­˜ç»“æœ
    """
    
    def __init__(self, 
                 model,
                 processor, 
                 data_args,
                 model_args,
                 device='cuda',
                 batch_size=8,
                 cirr_data_dir=None,
                 cirr_image_dir=None):
        self.model = model
        self.processor = processor
        self.data_args = data_args
        self.model_args = model_args
        self.device = device
        self.batch_size = batch_size
        
        # Get model backbone
        self.model_backbone = getattr(model_args, 'model_backbone', 'qwen2_vl')
        
        # Configure CIRR data paths
        self._configure_data_paths(cirr_data_dir, cirr_image_dir)
        
        # Load CIRR test data
        self.test_data, self.candidate_images = self._load_cirr_test_data()
        
        print_master(f"åŠ è½½äº† {len(self.test_data)} ä¸ªæŸ¥è¯¢")
        print_master(f"åŠ è½½äº† {len(self.candidate_images)} ä¸ªå€™é€‰å›¾åƒ")
    
    def _configure_data_paths(self, cirr_data_dir=None, cirr_image_dir=None):
        """é…ç½®CIRRæ•°æ®é›†è·¯å¾„"""
        # ä½¿ç”¨æä¾›çš„è·¯å¾„æˆ–é»˜è®¤è·¯å¾„
        if cirr_data_dir:
            self.data_dir = cirr_data_dir
        else:
            self.data_dir = '/home/guohaiyun/yty_data/CIRR/cirr'
        
        if cirr_image_dir:
            self.image_base_dir = cirr_image_dir
        else:
            self.image_base_dir = '/home/guohaiyun/yty_data/CIRR'
        
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        self.captions_file = os.path.join(self.data_dir, 'captions/cap.rc2.val.json')
        self.image_splits_file = os.path.join(self.data_dir, 'image_splits/split.rc2.val.json')
        
        print_master(f"ä½¿ç”¨CIRRæ•°æ®ç›®å½•: {self.data_dir}")
        print_master(f"ä½¿ç”¨CIRRå›¾åƒç›®å½•: {self.image_base_dir}")
    

    def _load_cirr_test_data(self) -> Tuple[List[Dict], List[str]]:
        """åŠ è½½CIRRéªŒè¯æ•°æ®"""
        try:
            if not os.path.exists(self.captions_file):
                print_master(f"è­¦å‘Š: CIRRéªŒè¯æŸ¥è¯¢æ–‡ä»¶æœªæ‰¾åˆ° {self.captions_file}")
                return self._create_dummy_test_data()
            
            # åŠ è½½éªŒè¯æŸ¥è¯¢
            with open(self.captions_file, 'r') as f:
                val_queries = json.load(f)
            
            # åŠ è½½éªŒè¯å›¾åƒåˆ†å‰²ä¿¡æ¯
            if os.path.exists(self.image_splits_file):
                with open(self.image_splits_file, 'r') as f:
                    val_splits = json.load(f)
                candidate_images = list(val_splits.keys())
                self.image_splits = val_splits
                print_master(f"ä»éªŒè¯åˆ†å‰²ä¸­åŠ è½½äº† {len(candidate_images)} ä¸ªå€™é€‰å›¾åƒ")
            else:
                print_master(f"è­¦å‘Š: éªŒè¯åˆ†å‰²æ–‡ä»¶æœªæ‰¾åˆ° {self.image_splits_file}")
                candidate_images = [f"dummy_img_{i}" for i in range(100)]
                self.image_splits = {}
            
            print_master(f"åŠ è½½äº† {len(val_queries)} ä¸ªCIRRéªŒè¯æŸ¥è¯¢")
            return val_queries, candidate_images
            
        except Exception as e:
            print_master(f"åŠ è½½CIRRéªŒè¯æ•°æ®æ—¶å‡ºé”™: {e}")
            return self._create_dummy_test_data()
    
    def _create_dummy_test_data(self) -> Tuple[List[Dict], List[str]]:
        """åˆ›å»ºè™šæ‹Ÿæµ‹è¯•æ•°æ®"""
        dummy_data = []
        for i in range(50):
            dummy_data.append({
                'pairid': i,
                'reference': f'dummy_ref_{i}',
                'target_hard': f'dummy_target_{i}',
                'caption': f'è™šæ‹Ÿä¿®æ”¹æ–‡æœ¬ {i}',
                'target_soft': {},
                'img_set': {'members': [f'dummy_img_{j}' for j in range(i, i+5)]}
            })
        candidate_images = [f"dummy_img_{i}" for i in range(100)]
        self.image_splits = {}
        return dummy_data, candidate_images
    
    
    def _encode_batch(self, batch_data: Dict[str, Any]) -> torch.Tensor:
        """
        ç›´æ¥å¤ç”¨CIRREvaluatorçš„ç¼–ç é€»è¾‘ï¼Œé¿å…é‡å¤å®ç°
        """
        try:
            # åˆ›å»ºä¸´æ—¶CIRREvaluatorå®ä¾‹æ¥å¤ç”¨å…¶ç¼–ç é€»è¾‘
            from src.evaluation.cirr_evaluator import CIRREvaluator
            
            # åˆ›å»ºä¸´æ—¶evaluatorå®ä¾‹ï¼Œå¤ç”¨å…¶æˆç†Ÿçš„ç¼–ç æ–¹æ³•
            temp_evaluator = CIRREvaluator(
                model=self.model,
                processor=self.processor,
                data_args=self.data_args,
                model_args=self.model_args,
                device=self.device,
                batch_size=self.batch_size
            )
            
            # ç›´æ¥è°ƒç”¨evaluatorçš„ç¼–ç æ–¹æ³•
            return temp_evaluator._encode_batch(batch_data)
                
        except Exception as e:
            print_master(f"ç¼–ç æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
            import traceback
            print_master(f"Traceback: {traceback.format_exc()}")
            # è¿”å›é›¶åµŒå…¥ä½œä¸ºå¤‡ç”¨
            return torch.zeros(len(batch_data['text']), 512, device=self.device)
    
    def _encode_images(self, image_names: List[str]) -> torch.Tensor:
        """ç¼–ç å€™é€‰å›¾åƒ - å¤ç”¨CIRREvaluatorçš„é€»è¾‘"""
        try:
            # åˆ›å»ºä¸´æ—¶CIRREvaluatorå®ä¾‹
            from src.evaluation.cirr_evaluator import CIRREvaluator
            temp_evaluator = CIRREvaluator(
                model=self.model,
                processor=self.processor,
                data_args=self.data_args,
                model_args=self.model_args,
                device=self.device,
                batch_size=self.batch_size
            )
            
            # ç›´æ¥è°ƒç”¨evaluatorçš„ç¼–ç æ–¹æ³•
            return temp_evaluator._encode_images_local(image_names)
        except Exception as e:
            print_master(f"ç¼–ç å€™é€‰å›¾åƒæ—¶å‡ºé”™: {e}")
            return torch.empty(0, 512, device=self.device)
    
    def _encode_composed_queries(self, queries: List[Dict]) -> torch.Tensor:
        """ç¼–ç å¤åˆæŸ¥è¯¢ - å¤ç”¨CIRREvaluatorçš„é€»è¾‘"""
        try:
            # åˆ›å»ºä¸´æ—¶CIRREvaluatorå®ä¾‹
            from src.evaluation.cirr_evaluator import CIRREvaluator
            temp_evaluator = CIRREvaluator(
                model=self.model,
                processor=self.processor,
                data_args=self.data_args,
                model_args=self.model_args,
                device=self.device,
                batch_size=self.batch_size
            )
            
            # ç›´æ¥è°ƒç”¨evaluatorçš„ç¼–ç æ–¹æ³•
            return temp_evaluator._encode_composed_queries_local(queries)
        except Exception as e:
            print_master(f"ç¼–ç å¤åˆæŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            return torch.empty(0, 512, device=self.device)
    
    def retrieve_top_k(self, top_k: int = 10, save_embeddings: bool = False) -> Dict[str, Any]:
        """
        å¯¹æ‰€æœ‰æŸ¥è¯¢è¿›è¡Œæ£€ç´¢å¹¶è¿”å›top-kç»“æœ
        
        Args:
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„top-kç»“æœæ•°é‡
            save_embeddings: æ˜¯å¦ä¿å­˜åµŒå…¥å‘é‡
        
        Returns:
            åŒ…å«æ‰€æœ‰æ£€ç´¢ç»“æœçš„å­—å…¸
        """
        print_master("å¼€å§‹CIRRæ£€ç´¢...")
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # ç¼–ç æ‰€æœ‰å€™é€‰å›¾åƒ
        candidate_embeddings = self._encode_images(self.candidate_images)
        candidate_embeddings = F.normalize(candidate_embeddings, p=2, dim=1)
        
        # ç¼–ç æ‰€æœ‰å¤åˆæŸ¥è¯¢
        query_embeddings = self._encode_composed_queries(self.test_data)
        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        print_master("è®¡ç®—ç›¸ä¼¼åº¦...")
        similarities = torch.mm(query_embeddings, candidate_embeddings.t())  # [num_queries, num_candidates]
        # ç¡®ä¿similaritiesä¸ºfloat32ç±»å‹ï¼Œé¿å…BFloat16è½¬numpyæ—¶å‡ºé”™
        similarities = similarities.float()
        
        # å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼Œæ’é™¤å‚è€ƒå›¾åƒï¼ˆé¿å…è‡ªæ£€ç´¢ï¼‰
        print_master("æ’é™¤å‚è€ƒå›¾åƒ...")
        for query_idx, query in enumerate(self.test_data):
            ref_image = query['reference']
            if ref_image in self.candidate_images:
                ref_idx = self.candidate_images.index(ref_image)
                similarities[query_idx, ref_idx] = -float('inf')
        
        # è·å–top-kç»“æœ
        print_master(f"è·å–æ¯ä¸ªæŸ¥è¯¢çš„top-{top_k}ç»“æœ...")
        _, top_k_indices = torch.topk(similarities, k=top_k, dim=1, largest=True)
        
        # æ„å»ºç»“æœ
        results = {
            'metadata': {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'model_path': getattr(self.model_args, 'checkpoint_path', 'unknown'),
                'model_backbone': self.model_backbone,
                'total_queries': len(self.test_data),
                'total_candidates': len(self.candidate_images),
                'top_k': top_k,
                'batch_size': self.batch_size,
                'device': str(self.device)
            },
            'queries': [],
            'candidate_images': self.candidate_images
        }
        
        # å¦‚æœéœ€è¦ä¿å­˜åµŒå…¥å‘é‡
        if save_embeddings:
            results['embeddings'] = {
                'query_embeddings': query_embeddings.cpu().numpy().tolist(),
                'candidate_embeddings': candidate_embeddings.cpu().numpy().tolist()
            }
        
        # ä¸ºæ¯ä¸ªæŸ¥è¯¢æ„å»ºè¯¦ç»†ç»“æœ
        print_master("æ„å»ºè¯¦ç»†ç»“æœ...")
        for query_idx, query in enumerate(tqdm(self.test_data, desc="å¤„ç†æŸ¥è¯¢ç»“æœ")):
            # è·å–top-kå€™é€‰å›¾åƒç´¢å¼•
            top_k_idx = top_k_indices[query_idx].cpu().numpy()
            
            # è·å–å¯¹åº”çš„ç›¸ä¼¼åº¦åˆ†æ•°
            top_k_scores = similarities[query_idx, top_k_idx].cpu().numpy()
            
            # æ„å»ºtop-kç»“æœåˆ—è¡¨
            retrieval_results = []
            for rank, (candidate_idx, score) in enumerate(zip(top_k_idx, top_k_scores)):
                candidate_name = self.candidate_images[candidate_idx]
                retrieval_results.append({
                    'rank': rank + 1,
                    'candidate_image': candidate_name,
                    'similarity_score': float(score),
                    'candidate_index': int(candidate_idx)
                })
            
            # æ„å»ºæŸ¥è¯¢ç»“æœ
            query_result = {
                'query_id': query_idx,
                'pairid': query['pairid'],
                'reference_image': query['reference'],
                'target_hard': query['target_hard'],
                'modification_text': query['caption'],
                'target_soft': query.get('target_soft', {}),
                'img_set': query.get('img_set', {}),
                'retrieval_results': retrieval_results
            }
            
            # æ·»åŠ ground truthä¿¡æ¯
            if 'target_hard' in query:
                # æ£€æŸ¥target_hardæ˜¯å¦åœ¨top-kç»“æœä¸­
                target_found = False
                target_rank = None
                for result in retrieval_results:
                    if result['candidate_image'] == query['target_hard']:
                        target_found = True
                        target_rank = result['rank']
                        break
                
                query_result['ground_truth'] = {
                    'target_hard': query['target_hard'],
                    'found_in_top_k': target_found,
                    'rank_in_top_k': target_rank
                }
            
            results['queries'].append(query_result)
        
        # è®¡ç®—ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        if 'ground_truth' in results['queries'][0]:
            found_count = sum(1 for q in results['queries'] if q['ground_truth']['found_in_top_k'])
            accuracy_at_k = found_count / len(results['queries'])
            results['metadata']['accuracy_at_k'] = accuracy_at_k
            results['metadata']['found_in_top_k_count'] = found_count
            print_master(f"Accuracy@{top_k}: {accuracy_at_k:.4f} ({found_count}/{len(results['queries'])})")
        
        print_master("æ£€ç´¢å®Œæˆ!")
        return results


def setup_device(device_arg: str) -> str:
    """è®¾ç½®å¹¶è¿”å›é€‚å½“çš„è®¾å¤‡"""
    if device_arg == "auto":
        if torch.cuda.is_available():
            device = "cuda"
            print_master(f"ä½¿ç”¨CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
        else:
            device = "cpu"
            print_master("ä½¿ç”¨CPUè®¾å¤‡")
    else:
        device = device_arg
        print_master(f"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")
    
    return device


def infer_model_name_from_path(model_path: str) -> str:
    """ä»æ£€æŸ¥ç‚¹è·¯å¾„æ¨æ–­åŸºç¡€æ¨¡å‹åç§°ï¼ˆä¸ eval_cirr.py ä¸€è‡´ï¼‰"""
    config_path = os.path.join(model_path, "config.json")
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            for key in ['_name_or_path', 'name_or_path', 'model_name', 'base_model_name']:
                if key in config and config[key]:
                    model_name = config[key]
                    print_master(f"ä»configæ¨æ–­åŸºç¡€æ¨¡å‹åç§°: {model_name}")
                    return model_name
        except Exception as e:
            print_master(f"è­¦å‘Š: è¯»å–config.jsonå¤±è´¥: {e}")
    path_lower = model_path.lower()
    if "qwen2-vl" in path_lower:
        if "2b" in path_lower:
            return "Qwen/Qwen2-VL-2B-Instruct"
        elif "7b" in path_lower:
            return "Qwen/Qwen2-VL-7B-Instruct"
    elif "llava" in path_lower:
        if "7b" in path_lower:
            return "llava-hf/llava-1.5-7b-hf"
        elif "13b" in path_lower:
            return "llava-hf/llava-1.5-13b-hf"
    print_master("è­¦å‘Š: æ— æ³•æ¨æ–­åŸºç¡€æ¨¡å‹åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    return "Qwen/Qwen2-VL-2B-Instruct"


def load_model_and_processor(retrieval_args: CIRRRetrievalArguments, model_args: ModelArguments, data_args: DataArguments):
    """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆå¤åˆ¶ eval_cirr.py çš„é€»è¾‘ï¼Œä¿è¯ä¸€è‡´æ€§ä¸ç¨³å®šæ€§ï¼‰"""
    print_master("=" * 60)
    print_master("åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨")
    print_master("=" * 60)

    # 1) åŸºç¡€æ¨¡å‹åç¡®å®š
    if retrieval_args.base_model_name:
        model_name = retrieval_args.base_model_name
        print_master(f"ä½¿ç”¨æä¾›çš„åŸºç¡€æ¨¡å‹åç§°: {model_name}")
    else:
        model_name = infer_model_name_from_path(retrieval_args.model_path)
        print_master(f"æ¨æ–­çš„åŸºç¡€æ¨¡å‹åç§°: {model_name}")

    # 2) å¤„ç† auto-infer
    if model_args.model_name == "auto-infer":
        model_args.model_name = model_name
        print_master(f"è‡ªåŠ¨æ¨æ–­çš„ model_name: {model_name}")
    elif not model_args.model_name:
        model_args.model_name = model_name

    model_args.checkpoint_path = retrieval_args.model_path

    # 3) è¦†ç›–å…³é”®é»˜è®¤å€¼ä»¥å¯¹é½è®­ç»ƒ
    print_master("è¦†ç›–ModelArgumentsé»˜è®¤å€¼ä»¥åŒ¹é…è®­ç»ƒé…ç½®...")
    model_args.pooling = 'eos'
    model_args.normalize = True
    print_master(f"âœ… è®¾ç½® pooling={model_args.pooling}, normalize={model_args.normalize}")
    print_master(f"âœ… è®¾ç½® lora_r={model_args.lora_r}, lora_dropout={model_args.lora_dropout}")

    data_args.max_len = 512
    data_args.resize_max_pixels = 262144 # 512*512
    print_master(f"âœ… è®¾ç½® max_len={data_args.max_len}, resize_max_pixels={data_args.resize_max_pixels}")

    # 4) LoRA / æœ¬åœ°é…ç½®æ£€æµ‹ï¼Œä¸ eval_cirr.py ä¸€è‡´
    local_config_path = os.path.join(retrieval_args.model_path, "config.json")
    adapter_config_path = os.path.join(retrieval_args.model_path, "adapter_config.json")

    if os.path.exists(adapter_config_path):
        print_master(f"æ‰¾åˆ°LoRAé€‚é…å™¨é…ç½®: {adapter_config_path}")
        try:
            with open(adapter_config_path, 'r') as f:
                adapter_config = json.load(f)
            if 'base_model_name_or_path' in adapter_config:
                base_model_name = adapter_config['base_model_name_or_path']
                print_master(f"LoRAåŸºç¡€æ¨¡å‹: {base_model_name}")
                model_args.model_name = base_model_name
                model_args.lora = True
                model_args.checkpoint_path = retrieval_args.model_path
                # ä¾æ®åŸºç¡€æ¨¡å‹ååšç®€å•çš„backboneæ¨æ–­ï¼ˆä¸ eval_cirr.py ä¿æŒä¸€è‡´ï¼‰
                lower = base_model_name.lower()
                if 'qwen2' in lower:
                    setattr(model_args, 'model_backbone', 'qwen2_vl')
                elif 'llava' in lower:
                    setattr(model_args, 'model_backbone', 'llava_next')
                else:
                    setattr(model_args, 'model_backbone', 'qwen2_vl')
                print_master(f"LoRA backbone: {model_args.model_backbone}")
            else:
                print_master("è­¦å‘Š: adapter_config ä¸­æœªæ‰¾åˆ° base_model_name_or_pathï¼Œä½¿ç”¨é»˜è®¤backbone qwen2_vl")
                setattr(model_args, 'model_backbone', 'qwen2_vl')
        except Exception as e:
            print_master(f"è¯»å–adapter_configå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤backbone qwen2_vl: {e}")
            setattr(model_args, 'model_backbone', 'qwen2_vl')
    elif os.path.exists(local_config_path):
        print_master(f"æ‰¾åˆ°æœ¬åœ°config.json: {local_config_path}")
        try:
            with open(local_config_path, 'r') as f:
                local_config = json.load(f)
            if 'model_type' in local_config:
                mt = local_config['model_type'].lower()
                if 'qwen2_vl' in mt:
                    setattr(model_args, 'model_backbone', 'qwen2_vl')
                elif 'llava' in mt:
                    setattr(model_args, 'model_backbone', 'llava_next')
                else:
                    setattr(model_args, 'model_backbone', 'qwen2_vl')
                print_master(f"ä»æœ¬åœ°configæ¨æ–­backbone: {model_args.model_backbone}")
            else:
                setattr(model_args, 'model_backbone', 'qwen2_vl')
                print_master(f"ä½¿ç”¨é»˜è®¤backbone: {model_args.model_backbone}")
        except Exception as e:
            setattr(model_args, 'model_backbone', 'qwen2_vl')
            print_master(f"è¯»å–æœ¬åœ°configå¤±è´¥ï¼Œé»˜è®¤backbone: {e}")
    else:
        print_master("æœªæ‰¾åˆ°æœ¬åœ°configæˆ–adapter_configï¼Œä½¿ç”¨é»˜è®¤backbone qwen2_vl")
        setattr(model_args, 'model_backbone', 'qwen2_vl')

    # 5) æŒ‰ eval è„šæœ¬æ–¹å¼åŠ è½½æ¨¡å‹
    model = None
    if getattr(model_args, 'lora', False):
        print_master("åŠ è½½LoRAæ¨¡å‹:")
        print_master(f"  åŸºç¡€æ¨¡å‹: {model_args.model_name}")
        print_master(f"  LoRAæ£€æŸ¥ç‚¹: {model_args.checkpoint_path}")
        try:
            model = MMEBModel.load(model_args, is_trainable=False)
            model.eval()
            print_master("âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print_master(f"âŒ åŠ è½½LoRAæ¨¡å‹å¤±è´¥: {e}")
            raise
    else:
        print_master(f"ä»æœ¬åœ°æ£€æŸ¥ç‚¹åŠ è½½å®Œæ•´æ¨¡å‹: {retrieval_args.model_path}")
        model_args.checkpoint_path = retrieval_args.model_path
        try:
            print_master("ä½¿ç”¨ MMEBModel.load åŠ è½½ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰...")
            model = MMEBModel.load(model_args, is_trainable=False)
            model.eval()
            print_master("âœ… å®Œæ•´æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print_master(f"âŒ å®Œæ•´æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    # 6) åŠ è½½å¤„ç†å™¨
    print_master("åŠ è½½å¤„ç†å™¨...")
    try:
        processor = load_processor(model_args, data_args)
        print_master("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
    except Exception as e:
        print_master(f"âŒ åŠ è½½å¤„ç†å™¨å¤±è´¥: {e}")
        raise

    setattr(model, 'processor', processor)
    print_master("=" * 60)
    return model, processor


def generate_output_filename(retrieval_args: CIRRRetrievalArguments) -> str:
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤ä¿å­˜åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ ./retrieval_results/ å­ç›®å½•ä¸­ï¼‰"""
    if retrieval_args.output_file:
        return retrieval_args.output_file
    
    # åŸºäºè„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç”Ÿæˆç›¸å¯¹é¡¹ç›®æ ¹ç›®å½•çš„è¾“å‡ºè·¯å¾„
    project_root = os.path.dirname(__file__)
    base_dir = os.path.join(project_root, 'retrieval_results')
    
    # ä»æ¨¡å‹è·¯å¾„ç”Ÿæˆç›®å½•å
    model_path = retrieval_args.model_path
    model_name = os.path.basename(model_path.rstrip('/'))
    
    # æ·»åŠ æ—¶é—´æˆ³ï¼Œè‡ªåŠ¨åˆ›å»ºæ–°çš„å­ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(base_dir, f"{model_name}_{timestamp}")
    
    # æœ€ç»ˆæ–‡ä»¶å
    filename = f"cirr_retrieval_top{retrieval_args.top_k}.json"
    
    return os.path.join(run_dir, filename)


def save_retrieval_results(results: Dict[str, Any], output_file: str):
    """ä¿å­˜æ£€ç´¢ç»“æœåˆ°JSONæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print_master(f"âœ… æ£€ç´¢ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ‰“å°æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(output_file)
        print_master(f"æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
        
    except Exception as e:
        print_master(f"âŒ ä¿å­˜æ£€ç´¢ç»“æœå¤±è´¥: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    parser = HfArgumentParser((CIRRRetrievalArguments, ModelArguments, DataArguments))
    
    if len(sys.argv) > 1 and sys.argv[1].endswith('.json'):
        # ä»JSONé…ç½®æ–‡ä»¶åŠ è½½
        retrieval_args, model_args, data_args = parser.parse_json_file(json_file=sys.argv[1])
    else:
        # ä»å‘½ä»¤è¡Œè§£æ
        retrieval_args, model_args, data_args = parser.parse_args_into_dataclasses()
    
    # éªŒè¯å‚æ•°
    if not retrieval_args.model_path:
        raise ValueError("model_pathæ˜¯å¿…éœ€çš„")
    
    if not os.path.exists(retrieval_args.model_path):
        raise ValueError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {retrieval_args.model_path}")
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(retrieval_args.device)
    
    # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    model, processor = load_model_and_processor(retrieval_args, model_args, data_args)
    
    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    model = model.to(device)
    print_master(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = CIRRRetriever(
        model=model,
        processor=processor,
        data_args=data_args,
        model_args=model_args,
        device=device,
        batch_size=retrieval_args.batch_size,
        cirr_data_dir=retrieval_args.cirr_data_dir,
        cirr_image_dir=retrieval_args.cirr_image_dir
    )
    
    # æ‰§è¡Œæ£€ç´¢
    results = retriever.retrieve_top_k(
        top_k=retrieval_args.top_k,
        save_embeddings=retrieval_args.save_embeddings
    )
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    output_file = generate_output_filename(retrieval_args)
    
    # ä¿å­˜ç»“æœ
    save_retrieval_results(results, output_file)
    
    print_master("ğŸ‰ CIRRæ£€ç´¢å®Œæˆ!")
    
    return results


if __name__ == "__main__":
    main()

# ç¤ºä¾‹å‘½ä»¤ï¼ˆä»…ä¾›å‚è€ƒï¼Œè¯·åœ¨ç»ˆç«¯æ‰§è¡Œï¼Œä¸è¦å†™åœ¨Pythonæ–‡ä»¶é‡Œï¼‰
# python retrieval_cirr.py \
#     --model_path /home/guohaiyun/yangtianyu/MyComposedRetrieval/experiments/IterativeCIRR_qwen2vl_20250918_191807/training_iter_0/checkpoint-1500 \
#     --base_model_name "Qwen/Qwen2-VL-7B-Instruct" \
#     --top_k 10 \
#     --batch_size 8 \
#     --device cuda \
#     --model_name auto-infer