#!/usr/bin/env python3
"""
CIRR æ£€ç´¢è„šæœ¬
- ä¸æ–°ç‰ˆ eval_cirr.pyã€cirr_evaluator.py å¯¹é½
- ç»Ÿä¸€æ¨¡å‹/å¤„ç†å™¨åŠ è½½ã€è®¾å¤‡è®¾ç½®ã€LoRA æ£€æµ‹ã€backbone æ¨æ–­
- é€šè¿‡ evaluator çš„ç»Ÿä¸€ç¼–ç æ¥å£ç”ŸæˆåµŒå…¥ï¼Œè®¡ç®—ç›¸ä¼¼åº¦å¹¶ä¿å­˜ top-k æ£€ç´¢ç»“æœ
"""

import os
import sys
import re
import json
import torch
import logging
import traceback
from datetime import datetime
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, field
from transformers import HfArgumentParser
import torch.nn.functional as F

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.arguments import ModelArguments, DataArguments
from src.model.model import MMEBModel
from src.model.processor import load_processor, get_backbone_name
from src.utils import print_rank, print_master


logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s [%(name)s:%(lineno)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


@dataclass
class CIRRRetrievalArguments:
    """CIRR æ£€ç´¢ä¸“ç”¨å‚æ•°"""
    model_path: str = field(
        metadata={"help": "è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (å¯ä»¥æ˜¯ checkpoint-xxx æˆ– iteration_x ç›®å½•)"}
    )
    base_model_name: str = field(
        default=None,
        metadata={"help": "åŸºç¡€æ¨¡å‹åç§° (ä¾‹å¦‚ Qwen/Qwen2-VL-2B-Instruct). è‹¥ä¸æä¾›ï¼Œå°†ä» model_path æ¨æ–­"}
    )
    output_file: str = field(
        default=None,
        metadata={"help": "ä¿å­˜æ£€ç´¢ç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„. è‹¥ä¸æä¾›ï¼Œå°†åŸºäºæ¨¡å‹è·¯å¾„è‡ªåŠ¨ç”Ÿæˆ"}
    )
    top_k: int = field(
        default=10,
        metadata={"help": "ä¿å­˜æ¯ä¸ªæŸ¥è¯¢çš„ top-k æ£€ç´¢ç»“æœ (é»˜è®¤: 10)"}
    )
    batch_size: int = field(
        default=8,
        metadata={"help": "æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8ï¼Œå¯è§†æ˜¾å­˜è°ƒæ•´)"}
    )
    device: str = field(
        default="auto",
        metadata={"help": "è®¾å¤‡: 'auto', 'cuda', 'cuda:0', ç­‰"}
    )
    cirr_data_dir: str = field(
        default=None,
        metadata={"help": "CIRR æ•°æ®é›†ç›®å½•è·¯å¾„ (åŒ…å« captions/ ä¸ image_splits/)"}
    )
    cirr_image_dir: str = field(
        default=None,
        metadata={"help": "CIRR å›¾åƒæ ¹ç›®å½•è·¯å¾„ (åŒ…å« dev/test/train å­ç›®å½•)"}
    )
    save_embeddings: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦åŒæ—¶ä¿å­˜æŸ¥è¯¢ä¸å€™é€‰å›¾åƒçš„åµŒå…¥å‘é‡ (JSON ä½“ç§¯è¾ƒå¤§æ—¶æ…ç”¨)"}
    )
    distributed: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼ (torchrun) è¿›è¡Œå¤šå¡ç¼–ç ä¸æ£€ç´¢"}
    )


class CIRRRetriever:
    """
    CIRR æ£€ç´¢å™¨
    - è´Ÿè´£ CIRR æ•°æ®åŠ è½½
    - é€šè¿‡ CIRREvaluator çš„ç¼–ç ç®¡çº¿è®¡ç®—åµŒå…¥
    - è®¡ç®—ç›¸ä¼¼åº¦å¹¶å¯¼å‡º top-k æ£€ç´¢ç»“æœ
    """

    def __init__(
        self,
        model,
        processor,
        data_args,
        model_args,
        device='cuda',
        batch_size=8,
        cirr_data_dir=None,
        cirr_image_dir=None,
        distributed: bool = False,
    ):
        self.model = model
        self.processor = processor
        self.data_args = data_args
        self.model_args = model_args
        self.device = device
        self.batch_size = batch_size
        self.distributed = distributed

        # backbone ä¸è¯„æµ‹ä¿æŒä¸€è‡´
        self.model_backbone = getattr(model_args, 'model_backbone', 'qwen2_vl')

        # é…ç½® CIRR æ•°æ®è·¯å¾„å¹¶åŠ è½½æ•°æ®
        self._configure_data_paths(cirr_data_dir, cirr_image_dir)
        self.test_data, self.candidate_images = self._load_cirr_test_data()

        # ä»…ç”¨äºå¤ç”¨ evaluator çš„ç»Ÿä¸€ç¼–ç æ¥å£
        try:
            from src.evaluation.cirr_evaluator import CIRREvaluator
            self._evaluator = CIRREvaluator(
                model=self.model,
                processor=self.processor,
                data_args=self.data_args,
                model_args=self.model_args,
                device=self.device,
                batch_size=self.batch_size,
            )
        except Exception as e:
            print_master(f"è­¦å‘Š: åˆ›å»º CIRREvaluator å¤±è´¥ï¼Œå°†å›é€€åˆ°ç®€åŒ–ç¼–ç : {e}")
            self._evaluator = None

        print_master(f"åŠ è½½äº† {len(self.test_data)} ä¸ªæŸ¥è¯¢")
        print_master(f"åŠ è½½äº† {len(self.candidate_images)} ä¸ªå€™é€‰å›¾åƒ")

    def _configure_data_paths(self, cirr_data_dir=None, cirr_image_dir=None):
        """é…ç½® CIRR æ•°æ®é›†è·¯å¾„"""
        # ä¸ evaluator é»˜è®¤ä¿æŒä¸€è‡´
        self.data_dir = cirr_data_dir or '/home/guohaiyun/yty_data/CIRR/cirr'
        self.image_base_dir = cirr_image_dir or '/home/guohaiyun/yty_data/CIRR'

        # æ–‡ä»¶è·¯å¾„
        self.captions_file = os.path.join(self.data_dir, 'captions/cap.rc2.val.json')
        self.image_splits_file = os.path.join(self.data_dir, 'image_splits/split.rc2.val.json')

        print_master(f"ä½¿ç”¨ CIRR æ•°æ®ç›®å½•: {self.data_dir}")
        print_master(f"ä½¿ç”¨ CIRR å›¾åƒç›®å½•: {self.image_base_dir}")

    def _load_cirr_test_data(self) -> Tuple[List[Dict], List[str]]:
        """åŠ è½½ CIRR éªŒè¯æ•°æ®"""
        try:
            if not os.path.exists(self.captions_file):
                print_master(f"è­¦å‘Š: æœªæ‰¾åˆ°éªŒè¯æŸ¥è¯¢æ–‡ä»¶ {self.captions_file}")
                return self._create_dummy_test_data()

            with open(self.captions_file, 'r') as f:
                val_queries = json.load(f)

            if os.path.exists(self.image_splits_file):
                with open(self.image_splits_file, 'r') as f:
                    val_splits = json.load(f)
                candidate_images = list(val_splits.keys())
                self.image_splits = val_splits
                print_master(f"ä»éªŒè¯åˆ†å‰²åŠ è½½ {len(candidate_images)} ä¸ªå€™é€‰å›¾åƒ")
            else:
                print_master(f"è­¦å‘Š: æœªæ‰¾åˆ°éªŒè¯åˆ†å‰²æ–‡ä»¶ {self.image_splits_file}")
                candidate_images = [f"dummy_img_{i}" for i in range(100)]
                self.image_splits = {}

            print_master(f"åŠ è½½ {len(val_queries)} ä¸ªéªŒè¯æŸ¥è¯¢")
            return val_queries, candidate_images
        except Exception as e:
            print_master(f"åŠ è½½ CIRR éªŒè¯æ•°æ®å¤±è´¥: {e}")
            return self._create_dummy_test_data()

    def _create_dummy_test_data(self) -> Tuple[List[Dict], List[str]]:
        """åˆ›å»ºè™šæ‹Ÿæ•°æ®ä»¥ä¾¿åœ¨æ— æ•°æ®æ—¶è°ƒè¯•"""
        dummy_data = []
        for i in range(50):
            dummy_data.append({
                'pairid': i,
                'reference': f'dummy_ref_{i}',
                'target_hard': f'dummy_target_{i}',
                'caption': f'è™šæ‹Ÿä¿®æ”¹æ–‡æœ¬ {i}',
                'target_soft': {},
                'img_set': {'members': [f'dummy_img_{j}' for j in range(i, i + 5)]},
            })
        candidate_images = [f"dummy_img_{i}" for i in range(100)]
        self.image_splits = {}
        return dummy_data, candidate_images

    def _encode_images(self, image_names: List[str]) -> torch.Tensor:
        """ç¼–ç å€™é€‰å›¾åƒï¼Œä¼˜å…ˆå¤ç”¨ evaluator ç»Ÿä¸€ç¼–ç æµç¨‹"""
        if self._evaluator is not None and hasattr(self._evaluator, '_encode_images_local'):
            return self._evaluator._encode_images_local(image_names)
        # å›é€€ï¼šè¿”å›ç©ºå¼ é‡é¿å…å´©æºƒ
        print_master("è­¦å‘Š: å›é€€åˆ°ç©ºå›¾åƒåµŒå…¥ï¼Œè¯·æ£€æŸ¥ evaluator å®ç°æ˜¯å¦å¯ç”¨")
        return torch.empty(0, 512, device=self.device)

    def _encode_composed_queries(self, queries: List[Dict]) -> torch.Tensor:
        """ç¼–ç å¤åˆæŸ¥è¯¢ï¼Œä¼˜å…ˆå¤ç”¨ evaluator ç»Ÿä¸€ç¼–ç æµç¨‹"""
        if self._evaluator is not None and hasattr(self._evaluator, '_encode_composed_queries_local'):
            return self._evaluator._encode_composed_queries_local(queries)
        # å›é€€ï¼šè¿”å›ç©ºå¼ é‡é¿å…å´©æºƒ
        print_master("è­¦å‘Š: å›é€€åˆ°ç©ºæŸ¥è¯¢åµŒå…¥ï¼Œè¯·æ£€æŸ¥ evaluator å®ç°æ˜¯å¦å¯ç”¨")
        return torch.empty(0, 512, device=self.device)

    def retrieve_top_k(self, top_k: int = 10, save_embeddings: bool = False) -> Dict[str, Any]:
        """
        å¯¹æ‰€æœ‰æŸ¥è¯¢è¿›è¡Œæ£€ç´¢å¹¶è¿”å› top-k ç»“æœ
        """
        print_master("å¼€å§‹ CIRR æ£€ç´¢...")
        self.model.eval()

        # 1) ç¼–ç å€™é€‰å›¾åƒä¸æŸ¥è¯¢ï¼ˆä¼˜å…ˆä½¿ç”¨åˆ†å¸ƒå¼ç¼–ç ï¼‰
        use_dist = False
        if self.distributed and self._evaluator is not None:
            try:
                import torch.distributed as dist
                use_dist = dist.is_initialized() and dist.get_world_size() > 1 \
                           and hasattr(self._evaluator, '_encode_images_distributed') \
                           and hasattr(self._evaluator, '_encode_queries_distributed')
            except Exception:
                use_dist = False

        if use_dist:
            print_master("ä½¿ç”¨åˆ†å¸ƒå¼ç¼–ç å€™é€‰å›¾åƒ...")
            candidate_embeddings = self._evaluator._encode_images_distributed()
            print_master("ä½¿ç”¨åˆ†å¸ƒå¼ç¼–ç æŸ¥è¯¢...")
            query_embeddings = self._evaluator._encode_queries_distributed()
        else:
            candidate_embeddings = self._encode_images(self.candidate_images)
            if candidate_embeddings.numel() == 0:
                print_master("âŒ å€™é€‰å›¾åƒåµŒå…¥ä¸ºç©ºï¼Œæ£€ç´¢ä¸­æ­¢")
                return {}
            query_embeddings = self._encode_composed_queries(self.test_data)
            if query_embeddings.numel() == 0:
                print_master("âŒ æŸ¥è¯¢åµŒå…¥ä¸ºç©ºï¼Œæ£€ç´¢ä¸­æ­¢")
                return {}

        # å½’ä¸€åŒ–
        candidate_embeddings = F.normalize(candidate_embeddings, p=2, dim=1)
        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)

        # 2) ç›¸ä¼¼åº¦
        print_master("è®¡ç®—ç›¸ä¼¼åº¦...")
        similarities = torch.mm(query_embeddings, candidate_embeddings.t()).float()

        # 3) æ’é™¤å‚è€ƒå›¾åƒ
        print_master("æ’é™¤å‚è€ƒå›¾åƒ...")
        image_to_idx = {img: idx for idx, img in enumerate(self.candidate_images)}
        for q_idx, q in enumerate(self.test_data):
            ref = q.get('reference')
            if ref in image_to_idx:
                similarities[q_idx, image_to_idx[ref]] = -float('inf')

        # 4) top-k
        k = min(top_k, similarities.size(1))
        print_master(f"è·å–æ¯ä¸ªæŸ¥è¯¢çš„ top-{k} ç»“æœ...")
        _, topk_idx = torch.topk(similarities, k=k, dim=1, largest=True)

        # 5) ç»„ç»‡ç»“æœ
        results: Dict[str, Any] = {
            'metadata': {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'model_path': getattr(self.model_args, 'checkpoint_path', 'unknown'),
                'model_backbone': getattr(self.model_args, 'model_backbone', 'unknown'),
                'total_queries': len(self.test_data),
                'total_candidates': len(self.candidate_images),
                'top_k': k,
                'batch_size': self.batch_size,
                'device': str(self.device),
                'distributed': bool(use_dist),
            },
            'queries': [],
            'candidate_images': self.candidate_images,
        }

        if save_embeddings:
            try:
                results['embeddings'] = {
                    'query_embeddings': query_embeddings.cpu().numpy().tolist(),
                    'candidate_embeddings': candidate_embeddings.cpu().numpy().tolist(),
                }
            except Exception as e:
                print_master(f"è­¦å‘Š: ä¿å­˜åµŒå…¥å¤±è´¥ï¼Œå·²è·³è¿‡ (åŸå› : {e})")

        print_master("æ„å»ºè¯¦ç»†ç»“æœ...")
        for q_idx, q in enumerate(self.test_data):
            indices = topk_idx[q_idx].tolist()
            scores = similarities[q_idx, indices].tolist()

            retrieval_results = []
            for rank, (ci, s) in enumerate(zip(indices, scores), start=1):
                retrieval_results.append({
                    'rank': rank,
                    'candidate_image': self.candidate_images[ci],
                    'similarity_score': float(s),
                    'candidate_index': int(ci),
                })

            qr: Dict[str, Any] = {
                'query_id': q_idx,
                'pairid': q.get('pairid', q_idx),
                'reference_image': q.get('reference'),
                'target_hard': q.get('target_hard'),
                'modification_text': q.get('caption'),
                'target_soft': q.get('target_soft', {}),
                'img_set': q.get('img_set', {}),
                'retrieval_results': retrieval_results,
            }

            tgt = q.get('target_hard')
            if tgt is not None:
                found = None
                for item in retrieval_results:
                    if item['candidate_image'] == tgt:
                        found = item['rank']
                        break
                qr['ground_truth'] = {
                    'target_hard': tgt,
                    'found_in_top_k': found is not None,
                    'rank_in_top_k': found,
                }

            results['queries'].append(qr)

        # 6) ç®€å•ç»Ÿè®¡
        if results['queries'] and 'ground_truth' in results['queries'][0]:
            found_cnt = sum(1 for q in results['queries'] if q['ground_truth']['found_in_top_k'])
            acc_at_k = found_cnt / len(results['queries']) if results['queries'] else 0.0
            results['metadata']['accuracy_at_k'] = acc_at_k
            results['metadata']['found_in_top_k_count'] = found_cnt
            print_master(f"Accuracy@{k}: {acc_at_k:.4f} ({found_cnt}/{len(results['queries'])})")

        print_master("æ£€ç´¢å®Œæˆ!")
        return results


def setup_device(device_arg: str, distributed: bool = False) -> str:
    """è®¾ç½®å¹¶è¿”å›è®¾å¤‡ (æ”¯æŒåˆ†å¸ƒå¼ torchrun)"""
    # åˆ†å¸ƒå¼åˆå§‹åŒ–ï¼ˆä»…å½“è®¾ç½®äº†ç¯å¢ƒå˜é‡æ—¶ï¼‰
    if distributed and ('RANK' in os.environ and 'WORLD_SIZE' in os.environ):
        try:
            import torch.distributed as dist
            if not dist.is_initialized():
                dist.init_process_group(backend='nccl')
            local_rank = int(os.environ.get('LOCAL_RANK', 0))
            if torch.cuda.is_available():
                torch.cuda.set_device(local_rank)
                device = f"cuda:{local_rank}"
                print_master(f"åˆ†å¸ƒå¼å·²åˆå§‹åŒ–: rank {dist.get_rank()}/{dist.get_world_size()}")
                print_master(f"ä½¿ç”¨è®¾å¤‡: {device}")
                return device
            else:
                print_master("CUDA ä¸å¯ç”¨ï¼Œå›é€€ CPU æ¨¡å¼")
        except Exception as e:
            print_master(f"åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€å•å¡æ¨¡å¼")

    # å•æœº/å•å¡
    if device_arg == 'auto':
        if torch.cuda.is_available():
            device = 'cuda'
            try:
                print_master(f"ä½¿ç”¨ CUDA è®¾å¤‡: {torch.cuda.get_device_name()}")
            except Exception:
                print_master("ä½¿ç”¨ CUDA è®¾å¤‡")
        else:
            device = 'cpu'
            print_master("CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
    else:
        device = device_arg
        print_master(f"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")
    return device


def infer_model_name_from_path(model_path: str, quiet: bool = False) -> str:
    """
    ä»æ£€æŸ¥ç‚¹è·¯å¾„æ¨æ–­åŸºç¡€æ¨¡å‹åç§°
    - å¢å¼ºï¼šæ”¯æŒ qwen2(.5|_5)?[-_]?vl ç­‰å¤šç§å†™æ³•
    - è‹¥å­˜åœ¨ config.json ä¼˜å…ˆè¯»å–
    """
    path_lower = model_path.lower()

    # ç›´æ¥åŒ¹é… qwen2(.5|_5)?[-_]?vl
    if re.search(r"qwen2(\.5|_5)?[-_]?vl", path_lower):
        is_qwen25 = bool(re.search(r"qwen2(\.5|_5)", path_lower))
        size = None
        if '2b' in path_lower:
            size = '2B'
        elif '7b' in path_lower:
            size = '7B'
        elif '32b' in path_lower:
            size = '32B'
        base = f"Qwen2.5-VL-{size or '7B'}-Instruct" if is_qwen25 else f"Qwen2-VL-{size or '7B'}-Instruct"
        model_name = f"/home/guohaiyun/yangtianyu/CPRCIR/checkpoints/hf_models/{base}"
        if not quiet:
            print_master(f"ä»è·¯å¾„æ¨¡å¼æ¨æ–­åŸºç¡€æ¨¡å‹: {model_name}")
        return model_name

    # è¯»å–æœ¬åœ° config.json
    config_path = os.path.join(model_path, 'config.json')
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r') as f:
                cfg = json.load(f)
            for key in ['_name_or_path', 'name_or_path', 'model_name', 'base_model_name']:
                if key in cfg and cfg[key]:
                    if not quiet:
                        print_master(f"ä» config æ¨æ–­åŸºç¡€æ¨¡å‹: {cfg[key]}")
                    return cfg[key]
        except Exception as e:
            if not quiet:
                print_master(f"è­¦å‘Š: è¯»å– config.json å¤±è´¥: {e}")

    default_name = 'Qwen/Qwen2-VL-2B-Instruct'
    if not quiet:
        print_master("è­¦å‘Š: æ— æ³•æ¨æ–­åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    return default_name


def load_model_and_processor(
    retrieval_args: CIRRRetrievalArguments,
    model_args: ModelArguments,
    data_args: DataArguments,
):
    """åŠ è½½æ¨¡å‹ä¸å¤„ç†å™¨ï¼Œé€»è¾‘ä¸ eval_cirr.py å¯¹é½"""
    print_master("=" * 60)
    print_master("åŠ è½½æ¨¡å‹ä¸å¤„ç†å™¨")
    print_master("=" * 60)

    # ä¼˜å…ˆä» LoRA é€‚é…å™¨è¯»å–åŸºç¡€æ¨¡å‹
    base_model_name = None
    lora_mode = False

    adapter_config_path = os.path.join(retrieval_args.model_path, 'adapter_config.json')
    if os.path.exists(adapter_config_path):
        try:
            with open(adapter_config_path, 'r') as f:
                ad_cfg = json.load(f)
            if 'base_model_name_or_path' in ad_cfg:
                base_model_name = ad_cfg['base_model_name_or_path']
                lora_mode = True
                print_master(f"æ£€æµ‹åˆ° LoRA é€‚é…å™¨ï¼ŒåŸºç¡€æ¨¡å‹: {base_model_name}")
        except Exception as e:
            print_master(f"è¯»å– adapter_config.json å¤±è´¥ (å°†å›é€€æ¨æ–­): {e}")

    # ç”¨æˆ·æ˜¾å¼æŒ‡å®šè¦†ç›–
    if retrieval_args.base_model_name:
        base_model_name = retrieval_args.base_model_name
        print_master(f"ä½¿ç”¨æä¾›çš„åŸºç¡€æ¨¡å‹åç§°: {base_model_name}")

    # æ¨æ–­
    if base_model_name is None:
        base_model_name = infer_model_name_from_path(retrieval_args.model_path, quiet=True)
        print_master(f"æ¨æ–­çš„åŸºç¡€æ¨¡å‹åç§°: {base_model_name}")

    # è®¾ç½® model_args.model_name
    if model_args.model_name in [None, 'auto-infer']:
        model_args.model_name = base_model_name
        print_master(f"æœ€ç»ˆ model_name: {model_args.model_name}")

    # checkpoint è·¯å¾„ä¸ LoRA æ ‡è®°
    model_args.checkpoint_path = retrieval_args.model_path
    model_args.lora = lora_mode or getattr(model_args, 'lora', False)

    # å…³é”®é»˜è®¤å€¼ä¸è®­ç»ƒå¯¹é½
    print_master("è¦†ç›– ModelArguments é»˜è®¤å€¼ä»¥åŒ¹é…è®­ç»ƒé…ç½®...")
    model_args.pooling = 'eos'
    model_args.normalize = True
    print_master(f"âœ… è®¾ç½® pooling={model_args.pooling}, normalize={model_args.normalize}")

    data_args.max_len = 512
    # ä¸æ–°ç‰ˆè¯„æµ‹é»˜è®¤åˆ†è¾¨ç‡ä¸€è‡´ (384x384 = 147456)
    data_args.resize_max_pixels = 147456
    print_master(f"âœ… è®¾ç½® max_len={data_args.max_len}, resize_max_pixels={data_args.resize_max_pixels}")

    # backbone æ£€æµ‹ï¼šä¼˜å…ˆ AutoConfig + get_backbone_name
    try:
        from transformers import AutoConfig
        base_cfg = AutoConfig.from_pretrained(model_args.model_name, trust_remote_code=True)
        detected_backbone = get_backbone_name(base_cfg, getattr(model_args, 'model_type', None))
        model_args.model_backbone = detected_backbone
        print_master(f"æ£€æµ‹åˆ° backbone: {detected_backbone}")
    except Exception as e_det:
        bl = model_args.model_name.lower()
        if 'qwen2.5' in bl or 'qwen2_5' in bl:
            model_args.model_backbone = 'qwen2_5_vl'
        elif 'qwen2' in bl:
            model_args.model_backbone = 'qwen2_vl'
        elif 'llava' in bl:
            model_args.model_backbone = 'llava_next'
        else:
            model_args.model_backbone = 'qwen2_vl'
        print_master(f"backbone æ£€æµ‹å›é€€ ({e_det}): {model_args.model_backbone}")

    # æ„å»º / åŠ è½½æ¨¡å‹
    model = None
    if model_args.lora:
        print_master("åŠ è½½ LoRA æ¨¡å‹ (base + adapter)...")
        try:
            model = MMEBModel.load(model_args, is_trainable=False)
            model.eval()
            print_master("âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print_master(f"âŒ LoRA æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    else:
        print_master("ä»æœ¬åœ°æ£€æŸ¥ç‚¹åŠ è½½å®Œæ•´æ¨¡å‹ (é LoRA)...")
        try:
            model = MMEBModel.load(model_args, is_trainable=False)
            model.eval()
            print_master("âœ… å®Œæ•´æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print_master(f"MMEBModel.load å¤±è´¥: {e}")
            print_master("å°è¯• build + æ‰‹åŠ¨åŠ è½½æƒé‡ å›é€€æ–¹æ¡ˆ...")
            try:
                original_ckpt = model_args.checkpoint_path
                model_args.checkpoint_path = None
                model = MMEBModel.build(model_args)

                # æŸ¥æ‰¾å¸¸è§æƒé‡æ–‡ä»¶
                weight_file = None
                if os.path.isdir(retrieval_args.model_path):
                    for f in ["pytorch_model.bin", "model.safetensors", "model.bin"]:
                        fp = os.path.join(retrieval_args.model_path, f)
                        if os.path.exists(fp):
                            weight_file = fp
                            break
                if weight_file is None:
                    raise ValueError(f"æœªåœ¨ {retrieval_args.model_path} æ‰¾åˆ°æƒé‡æ–‡ä»¶")

                print_master(f"ä» {weight_file} åŠ è½½æƒé‡...")
                if weight_file.endswith('.safetensors'):
                    from safetensors import safe_open
                    sd = {}
                    with safe_open(weight_file, framework='pt', device='cpu') as sf:
                        for k in sf.keys():
                            sd[k] = sf.get_tensor(k)
                else:
                    sd = torch.load(weight_file, map_location='cpu')

                model.load_state_dict(sd, strict=False)
                print_master("âœ… æƒé‡å·²åŠ è½½åˆ°æ„å»ºçš„æ¨¡å‹ä¸­")
                model_args.checkpoint_path = original_ckpt
            except Exception as e2:
                print_master(f"âŒ å›é€€æ–¹æ¡ˆå¤±è´¥: {e2}")
                raise

    # å¤„ç†å™¨
    print_master("åŠ è½½å¤„ç†å™¨...")
    try:
        processor = load_processor(model_args, data_args)
        print_master("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
    except Exception as e:
        print_master(f"âŒ åŠ è½½å¤„ç†å™¨å¤±è´¥: {e}")
        raise

    setattr(model, 'processor', processor)
    print_master("=" * 60)
    return model, processor


def generate_output_filename(retrieval_args: CIRRRetrievalArguments) -> str:
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤å†™å…¥ ./retrieval_results/<model>_<ts>/cirr_retrieval_topK.json)"""
    if retrieval_args.output_file:
        return retrieval_args.output_file

    project_root = os.path.dirname(__file__)
    base_dir = os.path.join(project_root, 'retrieval_results')

    model_path = retrieval_args.model_path.rstrip('/')
    model_name = os.path.basename(model_path)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(base_dir, f"{model_name}_{timestamp}")
    filename = f"cirr_retrieval_top{retrieval_args.top_k}.json"
    return os.path.join(run_dir, filename)


def save_retrieval_results(results: Dict[str, Any], output_file: str):
    """ä¿å­˜æ£€ç´¢ç»“æœåˆ° JSON æ–‡ä»¶"""
    try:
        out_dir = os.path.dirname(output_file)
        if out_dir:
            os.makedirs(out_dir, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print_master(f"æ£€ç´¢ç»“æœå·²ä¿å­˜: {output_file}")
    except Exception as e:
        print_master(f"âŒ ä¿å­˜æ£€ç´¢ç»“æœå¤±è´¥: {e}")
        print_master(f"Traceback: {traceback.format_exc()}")


def main():
    """ä¸»æµç¨‹ï¼šè§£æå‚æ•° -> è®¾ç½®è®¾å¤‡ -> åŠ è½½æ¨¡å‹/å¤„ç†å™¨ -> æ£€ç´¢ -> ä¿å­˜ç»“æœ"""
    parser = HfArgumentParser((CIRRRetrievalArguments, ModelArguments, DataArguments))

    if len(sys.argv) > 1 and sys.argv[1].endswith('.json'):
        retrieval_args, model_args, data_args = parser.parse_json_file(json_file=sys.argv[1])
    else:
        retrieval_args, model_args, data_args = parser.parse_args_into_dataclasses()

    if not retrieval_args.model_path or not os.path.exists(retrieval_args.model_path):
        raise ValueError(f"æ— æ•ˆçš„æ¨¡å‹è·¯å¾„: {retrieval_args.model_path}")

    device = setup_device(retrieval_args.device, retrieval_args.distributed)

    model, processor = load_model_and_processor(retrieval_args, model_args, data_args)
    model = model.to(device)
    print_master(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")

    retriever = CIRRRetriever(
        model=model,
        processor=processor,
        data_args=data_args,
        model_args=model_args,
        device=device,
        batch_size=retrieval_args.batch_size,
        cirr_data_dir=retrieval_args.cirr_data_dir,
        cirr_image_dir=retrieval_args.cirr_image_dir,
        distributed=retrieval_args.distributed,
    )

    results = retriever.retrieve_top_k(top_k=retrieval_args.top_k, save_embeddings=retrieval_args.save_embeddings)

    # ä»…ä¸»è¿›ç¨‹ä¿å­˜
    is_main = True
    if retrieval_args.distributed:
        try:
            import torch.distributed as dist
            if dist.is_initialized():
                is_main = (dist.get_rank() == 0)
        except Exception:
            is_main = True

    if results and is_main:
        output_file = generate_output_filename(retrieval_args)
        save_retrieval_results(results, output_file)
        print_master("ğŸ‰ CIRR æ£€ç´¢å®Œæˆï¼")

    # åˆ†å¸ƒå¼æ¸…ç†
    if retrieval_args.distributed:
        try:
            import torch.distributed as dist
            if dist.is_initialized():
                dist.barrier()
                dist.destroy_process_group()
        except Exception:
            pass

    return results if results else {}


if __name__ == "__main__":
    main()

# è¿è¡Œç¤ºä¾‹ï¼ˆåœ¨ bash ç»ˆç«¯æ‰§è¡Œï¼Œä»¥ä¸‹å‘½ä»¤å·²æ³¨é‡Šï¼‰
# 1) å•å¡ - å®Œæ•´æƒé‡ç›®å½•ï¼ˆé LoRAï¼‰
# python retrieval_cirr.py \
#   --model_path /home/guohaiyun/yangtianyu/MyComposedRetrieval/experiments/IterativeCIRR_qwen2vl_20250918_191807/training_iter_0/checkpoint-1500 \
#   --device cuda \
#   --top_k 10 \
#   --batch_size 8 \
#   --model_name auto-infer
#
# 2) å•å¡ - LoRA é€‚é…å™¨ç›®å½•ï¼ˆadapter_config.json å« base_model_name_or_pathï¼‰
# python retrieval_cirr.py \
#   --model_path /path/to/lora_adapter \
#   --device cuda \
#   --model_name auto-infer
#
#    å¦‚æœ adapter_config.json ç¼ºå°‘ base_model_name_or_pathï¼Œæ‰‹åŠ¨æŒ‡å®šåŸºç¡€æ¨¡å‹ï¼š
# python retrieval_cirr.py \
#   --model_path /path/to/lora_adapter \
#   --base_model_name Qwen/Qwen2-VL-7B-Instruct \
#   --device cuda \
#   --model_name auto-infer
#
# 3) è‡ªå®šä¹‰æ•°æ®è·¯å¾„ä¸ä¿å­˜åµŒå…¥ï¼ˆæ–‡ä»¶ä¼šè¾ƒå¤§ï¼‰
# python retrieval_cirr.py \
#   --model_path /path/to/checkpoint-1500 \
#   --cirr_data_dir /home/guohaiyun/yty_data/CIRR/cirr \
#   --cirr_image_dir /home/guohaiyun/yty_data/CIRR \
#   --save_embeddings \
#   --device cuda \
#   --model_name auto-infer
#
# 4) å¤šå¡åˆ†å¸ƒå¼ï¼ˆä¾‹å¦‚ 8 å¡ï¼‰
# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun --nproc_per_node=8 retrieval_cirr.py \
#   --model_path /path/to/checkpoint-1500 \
#   --device cuda \
#   --batch_size 8 \
#   --top_k 50 \
#   --distributed True \
#   --model_name auto-infer